## **1. 联邦学习的背景**

联邦学习是近年来兴起的一种**分布式机器学习技术**，其提出背景是现实生活中**数据难以集中管理**、**隐私安全问题**突出以及机器学习算法本身的局限性。[What is Federated Learning, 联邦学习(一、联邦学习概述) - 知乎](https://zhuanlan.zhihu.com/p/611394736)

## **2. 联邦学习的概念**

联邦学习：**联邦学习**是一种**分布式机器学习模型**，本质上是通过多个用户设备**共同训练**一个代表**所有用户设备**的**全局模型**，而训练的过程**不需要用户数据的交换**，较常见的分布式机器学习而言更**强调隐私性**。

### **2.1  Characteristics of FL**
Universality for cross-organizational scenarios

Massively Non-Identically Independent Distribution (Non-IID)

Decentralized technology

Equality of status for each node

### **2.2 Unique characteristics and issues of FL**
1) Slow and unstable communication
2) Heterogeneous devices
3) Privacy and security concerns

## **3. 联邦学习的分类**

不同类型的联邦学习方法主要根据**数据分布**和**数据拥有者之间的差异**而划分。主要分为**横向**联邦学习、**纵向**联邦学习和**迁移**联邦学习。

### **3.1 横向联邦学习**

**横向联邦学习的概念**

#横向联邦学习
**横向联邦学习**：每个参与者之间的数据源之间的**特征相同**，但数据**分布不同**。例如，多家医院共同合作来训练一个用于癌症诊断的模型，所有的患者都符合""癌症"这个病症的数据特征，但是每家医院可能拥有不同的患者数据。

>  In the case of horizontal FL, there is a certain amount of overlap between the feature of data spread across various nodes, while the data are quite different in sample space.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=10&selection=31,1,33,26|CAIE_A review of applications in federated learning_deposit, page 10]]

**横向联邦学习的训练过程：**

横向联邦学习可以看作是一种**数据并行**的分布式机器学习框架。即每个用户设备上都有一个用户自己的数据集，用户设备上持有的**用户数据是不同的**，而每个用户设备都有训练**一个相同的模型**。这个模型与服务器上的全局模型是一致的。

每个客户端在**本地**用自己的数据去**训练**这个模型，得到的模型参数通过网络信道**发送给服务器**。服务器对所有（或部分）用户设备的模型进行**聚合**，得到精度较高的、符合多个用户利益的**全局模型**。

在这个过程中，用户之间没有数据交换，保障了用户之间的**隐私性**。且每个用户都可以作为一个独立的个体，在本地运行经过协同训练后的模型，提高了**模型的性能**。

**训练步骤**如下：

1. 各个用户设备从服务器**下载**最新的模型。
2. 每个用户设备用自己的数据在本地进行模型**训练**。
3. 各个用户设备将本地迭代好的模型**上传**给服务器。
4. 服务器收集发送上来的所有模型，进行**模型聚合**。
5. 继续重复上述步骤，直至训练停止。

**横向联邦学习挑战及前人的对策**
#### **1.limited labeled entities**
>  Gao et al., (2019) introduced hierarchical heterogeneous horizontal FL frame. 

[[CAIE_A review of applications in federated learning_deposit.pdf#page=11&selection=6,17,7,10|CAIE_A review of applications in federated learning_deposit, page 11]]



### **3.2 纵向联邦学习**

**纵向联邦学习的概念**

#纵向联邦学习
**纵向联邦学习**：每个参与者之间的数据源之间的**特征不同**，但数据**分布相同**。例如，不同保险公司之间合作训练一个用于预测疾病风险的模型，其中每家保险公司可能只有一部分特征数据，但这些公司的样本数据都来自同一个数据源。

>  All the parties hold homogeneous data which means they have partial overlap on sample ID whereas differ in feature space.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=11&selection=20,50,22,6|CAIE_A review of applications in federated learning_deposit, page 11]]

**纵向联邦学习的训练过程**

纵向联邦学习本质上是多个参与方**共同完成一件事情**，把这件事情拆成多个部分去逐一分配给每个参与方。可以看作是一种**模型并行**的分布式机器学习框架。即每个用户设备上都有模型的一个部分，训练时需要用**同一批量数据**去让每个用户逐一得出自己那一部分的结果，然后按顺序将最终结果推导出来。

**训练步骤**如下：

1. 所有用户设备进行**数据对齐**，选择**同一个批量**的数据。
2. 将数据按**特征**分成不同的切片，每部分切片**下放**给对应的用户设备。
3. 用户设备拿到自己那部分的数据特征，用本地的模型分片进行**训练**。
4. 全局模型用每一个模块**聚合**完整的模型
5. 全局模型将每个模块的更新方式**下放**给每个参与者，参与者们完成本地更新
6. 重复以上步骤，直至训练结束。

**纵向联邦学习挑战及前人的对策**
#### **1.the correspondence between different owners**
> a modified token-based entity resolution algorithm to preprocess vertical partitioned data, powered by Nock et al., (2018).

[[CAIE_A review of applications in federated learning_deposit.pdf#page=12&selection=18,43,19,82|CAIE_A review of applications in federated learning_deposit, page 12]]

> Hardy et al., (2017) designed an end-to-end scheme on linear classifier and applied additive homomorphic encryption to defense honest-but-curious adversary for vertical FL. 

[[CAIE_A review of applications in federated learning_deposit.pdf#page=12&selection=20,0,22,4|CAIE_A review of applications in federated learning_deposit, page 12]]

### **3.3 迁移联邦学习**

**迁移联邦学习的概念**

#迁移联邦学习
**迁移联邦学习**：当每个参与者之间的数据源之间的**特征部分重叠**，**数据分布**也部分**重叠**时，以提高模型的泛化能力。例如，将在一个数据分布上训练的模型应用于另一个数据分布上，以提高模型在新数据上的表现。

> data shares neither sample space nor feature space

[[CAIE_A review of applications in federated learning_deposit.pdf#page=13&selection=2,12,2,62|CAIE_A review of applications in federated learning_deposit, page 13]]

**迁移学习简介**

迁移学习是指将从一个任务中学习到的知识或经验应用到另一个任务中的机器学习技术。

迁移学习的本质是通过利用源领域和目标领域之间的相关性，将源领域中学到的知识和经验应用于目标领域中，从而提高目标任务的性能。通常情况下，源领域和目标领域存在一定的相似性，例如数据分布、特征空间、任务目标等方面的相似性。

**联邦迁移学习的步骤**

迁移联邦学习本质上是学习**不同参与方**上的样本特征的**共同之处**。**迁移联邦学习**与**纵向联邦学习**的**结构**完全相同，不同点是**传递的数据不同**。纵向联邦学习只需要每个参与方关注自己本地的训练及更新结果，而由于迁移联邦学习需要多个参与方协作学习数据中的共同特征，因此数据传输时需要参与方彼此先沟通好，得到数据中的共同特征后。再通过服务器辨别共同特征和非共同特征后进行全局模型的学习。

**迁移联邦学习挑战及前人的对策**
#### **1.lack of data labels with poor data quality**
>  Thus, the main problem in this setting is lack of data labels with poor data quality.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=13&selection=2,63,3,70|CAIE_A review of applications in federated learning_deposit, page 13]]

#### **2.communication efficiency**
> Sharma et al., (2019) work hard on improvement for FTL. They made use of secret sharing technology instead of HE to further reduce overhead without decreasing the accurate rate.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=13&selection=13,42,15,61|CAIE_A review of applications in federated learning_deposit, page 13]]



## **4.Federated Averaging(FedAvg)**
[What is federated averaging (FedAvg)?](https://www.educative.io/answers/what-is-federated-averaging-fedavg)

![[Pasted image 20231023190854.png]]

```
Server:
1. Initialize global model parameters w.
2. for each round t do:
      a. Broadcast w to all clients.
      b. Collect updates δw from each client.
      c. Update global model parameters: w = Σ(δw) / number of clients.

Client:
1. Receive global model parameters w from the server.
2. for each local epoch do:
      a. Sample a batch of local data.
      b. Compute local gradient δw_local using SGD.
      c. Send δw_local to the server.
```

###  **Code example**
```
import numpy as np

# Define a sample model for demonstration
class Model:
    def __init__(self):
        self.fc = np.random.randn(10, 1)
    
    def forward(self, x):
        return np.dot(x, self.fc)  

# Client-side training function
def train_local_model(client_data, model):
    num_epochs = 10
    learning_rate = 0.1

    for epoch in range(num_epochs):
        inputs, labels = client_data  
        inputs = inputs.reshape(1, -1)  
        outputs = model.forward(inputs)
        loss = np.mean((outputs - labels) ** 2)
        grad = 2 * np.dot(inputs.T, outputs - labels) / inputs.shape[1]  
        model.fc -= learning_rate * grad

    return model

# Server-side federated averaging
def federated_averaging(global_model, client_data, num_rounds):
    for round in range(num_rounds):
        # Client selection
        selected_clients = np.random.choice(range(len(client_data)), size=3, replace=False)  # Select 3 client indices
        
        # Model distribution
        client_models = [global_model] * len(selected_clients)  # Provide duplicate global model to each client
        
        # Local training
        for i, client_index in enumerate(selected_clients):
            client_model = train_local_model(client_data[client_index], client_models[i])
            client_models[i] = client_model
        
        # Model aggregation
        aggregated_model = Model()
        for client_model in client_models:
            aggregated_model.fc += client_model.fc
        
        # Model averaging
        global_model.fc = aggregated_model.fc / len(client_models)
        
        print(f"Round {round+1} - Global Model Parameters:")
        print(global_model.fc)
        print()

# Example usage
global_model = Model()
client_data = [
    (np.random.randn(10), np.random.randn(1)),
    (np.random.randn(10), np.random.randn(1)),
    (np.random.randn(10), np.random.randn(1))
]  # Dummy client data
num_rounds = 2
federated_averaging(global_model, client_data, num_rounds)
```

### **How FedAvg works**
1. Initialization: First, the central server initializes the global model parameters and broadcasts them to all the client devices participating in the training process.
    
2. Client Training: Each client device trains the global model using its local data. The training is usually done using a standard optimization algorithm like Stochastic Gradient Descent (SGD). However, in FedAvg, the client does not train the model for multiple iterations. Instead, it trains the model for a few local epochs, typically referred to as "E", and then sends the updated model parameters back to the central server.
    
3. Model Aggregation: Once the client devices have completed their local training, the central server receives the model updates from each client. It then aggregates the model updates by computing the average of the model parameters received from the client devices.
    
4. Global Model Update: The central server updates the global model by replacing its current parameters with the averaged parameters obtained from the client devices.
    
5. Iteration: The above steps are repeated for multiple rounds, allowing the central server and the client devices to iteratively train the model and share information to improve the global model's performance. The iterative process helps the global model to benefit from the knowledge present in the local client data while preserving data privacy.

### **Benefits**

The federated averaging technique has the following benefits:

- Allows collaboration without providing raw data while maintaining privacy.
    
- Reduces communication costs because only model updates are sent between clients and the server.
    
- Scalable for large-scale machine learning applications.
    
- Minimizes the need for data transfer, reducing network bandwidth requirements and latency.
    
- Optimizes resource utilization, as it distributes the computational load across multiple devices or servers.
    

###  **Drawbacks**

The federated averaging has the following drawbacks:

- Data distribution heterogeneity between devices.
    
- Centralized control of the training process is lacking.
    
- Limited access to the data kept on specific servers or devices.
    
- Potential threats to privacy and security if necessary precautions are not taken.
    

### **Use cases**

The federated averaging technique is used in many different fields. It applies to:

- **Healthcare:** To train models using distributed patient data while maintaining privacy.
    
- **Finance sector:** It allows financial firms to collaborate without disclosing confidential customer data.
    
- **Edge computing relevance:** It is also helpful in edge computing applications (IoT devices, smartphones, or edge servers), where devices with a limited connection can contribute to model training.

## **5.目前的问题和解决办法**
> increased parallelism does not lead to significant improvements in reducing communication cost

[[Federated Learning in Mobile Edge Networks.pdf#page=10&selection=32,30,34,3|Federated Learning in Mobile Edge Networks, page 10]]
### **5.1 Communication-efficiency**

![[Pasted image 20231030203554.png]]

#### **5.1.1 Local Updating**
> distributed local-updating primal-dual methods have emerged as a popular way to tackle such a problem [54 , 62 , 72 , 107 , 128 ].

[[Federated Learning Challenges, Methods, and Future Directions.pdf#page=5&selection=83,51,107,2|Federated Learning Challenges, Methods, and Future Directions, page 5]]

#### **5.1.2 Compression Schemes**
> forcing the updating models to be sparse and low-rank; performing quantization with structured random rotations [ 59 ]; using lossy compression and dropout to reduce server-to-device communication [ 15 ]; and applying Golomb lossless encoding [ 99 ].

[[Federated Learning Challenges, Methods, and Future Directions.pdf#page=6&selection=37,66,51,2|Federated Learning Challenges, Methods, and Future Directions, page 6]]

Motivated by the limited resources of current devices in terms of compute, memory and communication, there are several different compression objectives of practical value.
> (a) Gradient compression6 – reduce the size of the object communicated from clients to server, which is used to update the global model.
> (b) Model broadcast compression – reduce the size of the model broadcast from server to clients, from which the clients start local training. 
> (c) Local computation reduction – any modification to the overall training algorithm such that the local training procedure is computationally more efficient.

[[Advances and Open Problems in Federated Learning.pdf#page=32&selection=36,0,54,52|Advances and Open Problems in Federated Learning, page 32]]

> Structured updates restrict participant updates to have a pre- specified structure, i.e., low rank and random mask. For the low rank structure, each update is enforced to be a low rank matrix expressed as a product of two matrices

[[Federated Learning in Mobile Edge Networks.pdf#page=11&selection=65,0,70,45|Federated Learning in Mobile Edge Networks, page 11]]

#### **5.1.3 Decentralized Training**
> Finally, hierarchical communication patterns have also been proposed [ 68 , 70 ] to further ease the burden on the central server, by first leveraging edge servers to aggregate the updates from edge devices and then relying on a cloud server to aggregate updates from edge servers. 

[[Federated Learning Challenges, Methods, and Future Directions.pdf#page=6&selection=96,3,115,9|Federated Learning Challenges, Methods, and Future Directions, page 6]]

#### **5.1.4 Reducing communication rounds**
> The research of McMahan et al., (2017) is considered as the pioneering work on FL to make communication more efficient by increasing calculated quantity on each client between each communication round. They also pointed out that increase parallelism which means motivate more clients to join training on each round is an effective way

[[CAIE_A review of applications in federated learning_deposit.pdf#page=15&selection=24,24,28,42|CAIE_A review of applications in federated learning_deposit, page 15]]

>  Inspired by Google, Nishio and Yonetani (2019) built FedCs framework to integrate the available clients to the utmost extent in each training round to make it efficiently in practice

[[CAIE_A review of applications in federated learning_deposit.pdf#page=15&selection=28,42,30,52|CAIE_A review of applications in federated learning_deposit, page 15]]

> Maximum mean discrepancy was inserted to FL algorithm to enforce local model to acquire more knowledge from other in training devices thus speed up convergence (Yao et al, 2018).

[[CAIE_A review of applications in federated learning_deposit.pdf#page=15&selection=30,52,30,80|CAIE_A review of applications in federated learning_deposit, page 15]]

> Yurochkin et al., (2019) designed Bayesian Nonparametric FL framework, which is state of the art since it can aggregate local models into a federated model without extra parameters thus avoid unwanted communication rounds. 

[[CAIE_A review of applications in federated learning_deposit.pdf#page=16&selection=1,69,4,42|CAIE_A review of applications in federated learning_deposit, page 16]]

#### **5.1.5 Decrease model update time**
##### **1.structured update**
> transmit only part of the update model by means of low-rank model or in a random mask way. 

[[CAIE_A review of applications in federated learning_deposit.pdf#page=16&selection=14,32,19,40|CAIE_A review of applications in federated learning_deposit, page 16]]

> an end-to-end neural network is a kind of structured update mode which maps update information into a lower- dimension space thus relieve pressure of communication (Li and Han, 2019).

[[CAIE_A review of applications in federated learning_deposit.pdf#page=16&selection=19,50,21,74|CAIE_A review of applications in federated learning_deposit, page 16]]

#### **2.sketched update**
>  refer to make use of compressed update model.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=16&selection=22,31,22,77|CAIE_A review of applications in federated learning_deposit, page 16]]

>  Zhu and Jin (2019) optimized sparse evolutionary training (SET) thus convey only piece of parameters to server, which resemble the sketched update. 

[[CAIE_A review of applications in federated learning_deposit.pdf#page=16&selection=22,77,24,61|CAIE_A review of applications in federated learning_deposit, page 16]]

>  Jiang and Ying (2020) designed an adaptive method for local training. The local training epochs is decided by server according to training time and training loss, thus it will reduce local training time when loss is getting small.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=16&selection=25,37,28,14|CAIE_A review of applications in federated learning_deposit, page 16]]

> Liu et al., (2020) utilized momentum gradient descent to consider previous gradient information in each local training epoch to accelerate convergence speed.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=16&selection=30,11,31,82|CAIE_A review of applications in federated learning_deposit, page 16]]

#### **5.1.6 Non-IID Data in Federated Learning**
##### **1.Non-identical client distributions**
> under feature-distribution skew, because P(y | x) is assumed to be common, the problem is at least in principle well specified, and training a single global model that learns P(y | x) may be appropriate

[[Advances and Open Problems in Federated Learning.pdf#page=19&selection=81,9,104,18|Advances and Open Problems in Federated Learning, page 19]]

> One approach is to create a small dataset which can be shared globally. 

[[Advances and Open Problems in Federated Learning.pdf#page=20&selection=3,9,3,81|Advances and Open Problems in Federated Learning, page 20]]

##### **2.Violations of independence**

##### **3.Dataset shift**

#### **5.1.7 Edge and End Computation**
> an edge computing inspired paradigm in which proximate edge servers can serve as intermediary parameter aggregators given that the propagation latency from participant to the edge server is smaller than that of the participant-cloud communication

[[Federated Learning in Mobile Edge Networks.pdf#page=10&selection=150,0,156,59|Federated Learning in Mobile Edge Networks, page 10]]

#### **5.1.8 Importance-based Updating**
> The eSGD algorithm keeps track of loss values at two consecutive training iterations. If the loss value of the current iteration is smaller than the preceding iteration, this implies that current training gradients and model parameters are important for training loss minimalization and thus, their respective hidden weights are assigned a positive value.

[[Federated Learning in Mobile Edge Networks.pdf#page=12&selection=40,54,46,38|Federated Learning in Mobile Edge Networks, page 12]]

当累积的残差梯度达到了一个阈值时，它们将被用来替代在模型训练中不太重要的梯度坐标。这可以看作是一种机制，确保小梯度值在积累后能够对模型参数进行更有针对性的更新，以提高训练效率。
> Once the accumulated residual gradient reaches a threshold, they are chosen to replace the least important gradient coordinates according to the hidden weight values

[[Federated Learning in Mobile Edge Networks.pdf#page=12&selection=58,47,61,37|Federated Learning in Mobile Edge Networks, page 12]]

### **5.2 Systems Heterogeneity**

#### **5.2.1 Asynchronous Communication**
> bounded-delay assumptions 

[[Federated Learning Challenges, Methods, and Future Directions.pdf#page=7&selection=81,36,81,61|Federated Learning Challenges, Methods, and Future Directions, page 7]]

#### **5.2.2 Active Sampling **
>  actively selecting participating devices

[[Federated Learning Challenges, Methods, and Future Directions.pdf#page=7&selection=132,95,136,31|Federated Learning Challenges, Methods, and Future Directions, page 7]]

#### **5.2.3 Fault Tolerance**
> Non-malicious failure modes (Section 5.2) are can be especially different to deal with, as access to raw data is not available in the federated setting, though through some lens they may be related to poisoning attacks.

[[Advances and Open Problems in Federated Learning.pdf#page=73&selection=53,2,55,18|Advances and Open Problems in Federated Learning, page 73]]

> One practical strategy is to simply ignore such device failure [ 11], which may introduce bias into the device sampling scheme if the failed devices have specific data characteristics.

[[Federated Learning Challenges, Methods, and Future Directions.pdf#page=8&selection=44,3,49,49|Federated Learning Challenges, Methods, and Future Directions, page 8]]

> Coded computation is another option to tolerate device failures by introducing algorithmic redundancy.

[[Federated Learning Challenges, Methods, and Future Directions.pdf#page=8&selection=71,1,73,84|Federated Learning Challenges, Methods, and Future Directions, page 8]]

>  Enable FL system to be robust to dropped participants, scholars also designed secure aggregation protocol (Haoa et al., 2019) which is tolerant with arbitrary dropouts as long as surviving users are enough to join federate update.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=19&selection=16,23,21,81|CAIE_A review of applications in federated learning_deposit, page 19]]

> Lib et al., (2019) take stragglers into account and allow these devices to implement different locally update computation times. 

[[CAIE_A review of applications in federated learning_deposit.pdf#page=19&selection=21,82,26,34|CAIE_A review of applications in federated learning_deposit, page 19]]

> Wu et al., (2019) also fully considered device straggling phenomenon in heterogeneous network. They made use of a cache structure to store those unreliable user update thus alleviates their trustless impact on global model.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=19&selection=26,35,27,48|CAIE_A review of applications in federated learning_deposit, page 19]]

#### **5.2.4 Resource allocation**

![[Pasted image 20231030205033.png]]

>  Kang et al., (2019) took overhead in heterogeneous clients into consideration to motivate more high-quality devices to participate training process.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=19&selection=34,77,36,53|CAIE_A review of applications in federated learning_deposit, page 19]]

> Tran et al., (2019) studied training accuracy and convergence time with influence of heterogeneous power constraints.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=19&selection=36,58,38,12|CAIE_A review of applications in federated learning_deposit, page 19]]

> Chai et al., (2019) considered the impact of resource (e.g. CPU, memory, and network resources) heterogeneity on training time of FL.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=19&selection=38,24,39,73|CAIE_A review of applications in federated learning_deposit, page 19]]

>  Li, T. et al., (2020) designed a fairness metrics to measure loss in devices and a q-Fair optimization goal to impel fair resource allocation in FL. 

[[CAIE_A review of applications in federated learning_deposit.pdf#page=19&selection=40,19,40,88|CAIE_A review of applications in federated learning_deposit, page 19]]

1) Participant Selection
2) Joint Radio and Computation Resource Management

 > To improve the model accuracy, the untransmitted gradient vectors can first be stored in an error accumulation vector. In the next round, local gradient estimates are then corrected using the error vector. 

[[Federated Learning in Mobile Edge Networks.pdf#page=16&selection=33,9,45,1|Federated Learning in Mobile Edge Networks, page 16]]
4) Adaptive Aggregation
FedAsync algorithm
通过根据本地更新的陈旧度来自适应地加权它们，允许那些相对较旧的本地更新在整体模型训练中仍然有所贡献。这确保了即使某些参与者的本地更新在时间上滞后，它们仍然能够尽可能地参与到整体训练中。
6) Incentive Mechanism

> well-designed contracts can reduce information asymmetry through self-revealing mechanisms in which partic- ipants select only the contracts specifically designed for their types.

[[Federated Learning in Mobile Edge Networks.pdf#page=17&selection=102,15,105,6|Federated Learning in Mobile Edge Networks, page 17]]

> a multi- dimensional contract in which each FL participant determines the optimal computation power and image quality it is willing to contribute for model training, in exchange for contract rewards in each iteration.

[[Federated Learning in Mobile Edge Networks.pdf#page=18&selection=20,53,28,26|Federated Learning in Mobile Edge Networks, page 18]]

> a reputation-based participant selection scheme for reliable FL

[[Federated Learning in Mobile Edge Networks.pdf#page=18&selection=31,0,31,61|Federated Learning in Mobile Edge Networks, page 18]]

### **5.3 Statistical Heterogeneity**

#### **5.3.1 Modeling Heterogeneous Data **
> MOCHA [ 106 ], an optimization framework designed for the federated setting, can allow for personalization by learning separate but related models for each device while leveraging a shared representation via multi-task learning.

[[Federated Learning Challenges, Methods, and Future Directions.pdf#page=8&selection=160,1,173,88|Federated Learning Challenges, Methods, and Future Directions, page 8]]

>  Another approach [ 26] models the star topology as a Bayesian network and performs variational inference during learning.

[[Federated Learning Challenges, Methods, and Future Directions.pdf#page=9&selection=22,64,26,82|Federated Learning Challenges, Methods, and Future Directions, page 9]]

>  q-FFL in which devices with higher loss are given higher relative weight to encourage less variance in the final accuracy distribution. 

[[Federated Learning Challenges, Methods, and Future Directions.pdf#page=9&selection=77,0,80,59|Federated Learning Challenges, Methods, and Future Directions, page 9]]

#### **5.3.2 Convergence Guarantees for Non-IID Data **
> FedProx [ 65 ] has recently been proposed

[[Federated Learning Challenges, Methods, and Future Directions.pdf#page=10&selection=47,1,54,28|Federated Learning Challenges, Methods, and Future Directions, page 10]]

#### **5.3.3 Convergence speed of a FL algorithm**
> urther Wang et al., (2019) discussed convergence bound of FL based on gradient-descent in Non-IID data background, and further bring forward an improved adaptive method to reduce loss function within constraints of resource budget.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=17&selection=26,33,29,16|CAIE_A review of applications in federated learning_deposit, page 17]]

> Li, X. et al (2019). gave four kinds of convergence theorems with different parameters setting or premises for FedAvg in Non-IID situations

[[CAIE_A review of applications in federated learning_deposit.pdf#page=17&selection=29,28,30,76|CAIE_A review of applications in federated learning_deposit, page 17]]

#### **5.3.4 Add extra data preprocessing procedure**
>  Huanga et al., (2019) introduced clustering thought with FL and constructed a community-based FL method. 

[[CAIE_A review of applications in federated learning_deposit.pdf#page=18&selection=5,26,10,9|CAIE_A review of applications in federated learning_deposit, page 18]]

> In hierarchical heterogeneous horizontal framework, it projects each embedding submanifold into a common embedding space to overcome data heterogeneity (Gao et al., 2019).

[[CAIE_A review of applications in federated learning_deposit.pdf#page=18&selection=15,31,17,47|CAIE_A review of applications in federated learning_deposit, page 18]]

#### **5.3.5 Modify local training mode**
>  Another idea is to optimize modeling way to achieve personalization for individual devices such as MOCHA, which introduced multi-task learning to make utilization of shared representation (Smith et al., 2017). 

[[CAIE_A review of applications in federated learning_deposit.pdf#page=18&selection=20,1,22,76|CAIE_A review of applications in federated learning_deposit, page 18]]

> Zhao et al., (2018) did the similar work, they considered a solution to deal with non-iid data by sharing a small set of data among each local model.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=18&selection=22,76,24,51|CAIE_A review of applications in federated learning_deposit, page 18]]

>  Huangb et al., (2019) also gained a good deal of enlightenment from the previous data sharing ideology to overcome Non-IID problem. 

[[CAIE_A review of applications in federated learning_deposit.pdf#page=18&selection=24,51,29,18|CAIE_A review of applications in federated learning_deposit, page 18]]


### **5.4 Privacy**

#### **5.4.1 Privacy in Machine Learning**
> differential privacy to communicate noisy data sketches, homomorphic encryption to operate on encrypted data

[[Federated Learning Challenges, Methods, and Future Directions.pdf#page=10&selection=175,8,176,14|Federated Learning Challenges, Methods, and Future Directions, page 10]]

> or gradient-based learning methods, a popular approach is to apply differential privacy by randomly perturbing the intermediate output at each iteration

[[Federated Learning Challenges, Methods, and Future Directions.pdf#page=10&selection=219,4,220,91|Federated Learning Challenges, Methods, and Future Directions, page 10]]

#### **5.4.2 Privacy in Federated Learning**
> a relaxed version of local privacy by limiting the power of potential adversaries. It affords stronger privacy guarantees than global privacy, and has better model performance than strict local privac

[[Federated Learning Challenges, Methods, and Future Directions.pdf#page=12&selection=10,10,12,24|Federated Learning Challenges, Methods, and Future Directions, page 12]]

>  locally differentially-private algorithms in the context of meta-learning, which can be applied to federated learning with personalization, while also providing provable learning guarantees in convex setting

[[Federated Learning Challenges, Methods, and Future Directions.pdf#page=12&selection=16,7,18,46|Federated Learning Challenges, Methods, and Future Directions, page 12]]

> is desirable to distinguish the view of the server administrator from the view of the analysts that consume the learned models

[[Advances and Open Problems in Federated Learning.pdf#page=37&selection=45,0,46,18|Advances and Open Problems in Federated Learning, page 37]]

Information exploiting attacks

> • Differentially private stochastic gradient descent: Add “noise” to the trained parameters by using a differential privacy-preserving randomized mechanism [20]. 
> • Differentially private and selective participants: Add “noise” to the trained parameters and select randomly participants to train global model in each round [161]. 
> • Selective parameter sharing: Each participant wisely selects the number of gradients to upload and the number of parameters from the global model to update [162]. 
> • Secrete sharing scheme with extreme boosting algorithm: This approach executes a lightweight secret sharing protocol before transmitting the newly trained model in plaintext to the server at each round [166]. 
> • GAN model training: All participants are cooperative to train a federated GANs model [167]

[[Federated Learning in Mobile Edge Networks.pdf#page=24&selection=18,0,46,72|Federated Learning in Mobile Edge Networks, page 24]]

#### **5.4.3  Privacy Risk**
![[328a95f6a216860d240fcc233fba5f9c_63_Table_11.png]]
##### **5.4.3.1 Data poisoning Attack**
> On the basis research of Bagdasaryan et al., (2018), Yang, Qb. et al., (2019) studied a novel and effective distributed backdoor attack. They divided an attack trigger into many slices and embedded each slice into different attackers instead of embedding a complete trigger into only one attacker

[[CAIE_A review of applications in federated learning_deposit.pdf#page=21&selection=12,7,17,51|CAIE_A review of applications in federated learning_deposit, page 21]]

> Distinguish honest participants based on their updated gradients. It is based on the fact that in the non-IID FL setting, each participant’s training data has its own particularities, and malicious participants will contribute gradients that appear more similar to each other than those of the honest participant

[[Federated Learning in Mobile Edge Networks.pdf#page=24&selection=60,2,63,25|Federated Learning in Mobile Edge Networks, page 24]]

##### **5.4.3.2 Model poisoning Attack**
> it can be subdivided into Non-targeted adversarial attack and Targeted adversarial attack

[[CAIE_A review of applications in federated learning_deposit.pdf#page=21&selection=26,20,27,27|CAIE_A review of applications in federated learning_deposit, page 21]]

> In FL, secure aggregation is implemented, and aggregator is not familiar with the local update modes thus are not able to detect anomalies or verify correctness of local updates

[[CAIE_A review of applications in federated learning_deposit.pdf#page=21&selection=29,25,31,35|CAIE_A review of applications in federated learning_deposit, page 21]]

> This novel attack method can be successfully employed in federated training tasks including image classification and word prediction (Bagdasaryan et al., 2018).

[[CAIE_A review of applications in federated learning_deposit.pdf#page=21&selection=33,60,34,79|CAIE_A review of applications in federated learning_deposit, page 21]]

> Zhang et al., (2019) give first attempt to generate model poisoning attack based on Generative Adversarial Nets (GAN). 

[[CAIE_A review of applications in federated learning_deposit.pdf#page=22&selection=5,67,7,13|CAIE_A review of applications in federated learning_deposit, page 22]]

> • Based on an updated model shared from a participant, the server can check whether the shared model can help to improve the global model’s performance or not. If not, the participant will be marked to be a potential attacker [173]. 
> • Compare among the updated global models shared by the participants, and if an updated global model from a participant is too different from others, it could be a potential malicious participant [173].

[[Federated Learning in Mobile Edge Networks.pdf#page=24&selection=70,0,86,18|Federated Learning in Mobile Edge Networks, page 24]]

##### **5.4.3.3 Inferring Attack**
>  Wang, Z. et al., (2019) built a general attack frame called mGAN-AI which could reconstruct private information for target client.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=22&selection=26,40,27,83|CAIE_A review of applications in federated learning_deposit, page 22]]

> Training participants can influence the optimization process possibly exacerbating inference-time (Section evasion attacks) 5.1.4, and communication and computation constraints may render pre- viously proposed defenses impractical.

[[Advances and Open Problems in Federated Learning.pdf#page=73&selection=47,3,52,38|Advances and Open Problems in Federated Learning, page 73]]

##### **5.4.3.4 Free-Riding Attack**
就是白嫖，只想自己获益但是不想贡献数据

> Participants’ local learning model updates are exchanged and verified by leveraging blockchain technology. In particular, each participant trains and sends the trained global model to its associated miner in the blockchain network and then receives a reward that is proportional to the number of trained data sample

[[Federated Learning in Mobile Edge Networks.pdf#page=24&selection=96,2,99,33|Federated Learning in Mobile Edge Networks, page 24]]

##### **5.4.3.5  privacy-preserving technology in FL**
>  Insider adversaries including honest-but-curious aggregator, colluding parties and malicious participants steal privacy during training process

[[CAIE_A review of applications in federated learning_deposit.pdf#page=23&selection=6,80,8,50|CAIE_A review of applications in federated learning_deposit, page 23]]

##### **5.4.3.6 Privacy-preserving at client side**
> since FedAvg is prone to be violated by differential attack, Geyer et al., (2019) leveraged differential privacy on FL to conceal whether a client participant in the training process.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=23&selection=24,63,26,70|CAIE_A review of applications in federated learning_deposit, page 23]]

> Faced with these vulnerabilities, the existing privacy-preserving methods to enhance privacy guarantees mainly focus on information encryption for client or secure aggregation at server side as well as security protection for FL framework (Ma et al., 2019).

差分攻击(DP)是一种通过分析模型输出的微小差异来推断个别客户的数据的攻击方式。

[[CAIE_A review of applications in federated learning_deposit.pdf#page=23&selection=13,50,16,55|CAIE_A review of applications in federated learning_deposit, page 23]]

>  to improve FedAvg, McMahan et al., (2018) also applied DP to this process by adding Gaussian noise to the global model. 

[[CAIE_A review of applications in federated learning_deposit.pdf#page=23&selection=26,80,28,36|CAIE_A review of applications in federated learning_deposit, page 23]]

> In federated online training for ranker using feedback from users, Kharitonov (2019) introduced ε-local differential privacy. Opposite to common algorithms

ε-局部差分隐私通常涉及到在本地计算中引入噪声或随机性，以增强个体隐私的保护。该方法更为严格，因为它在用户级别（个体级别）保护隐私，而不是在数据聚合后才应用隐私保护技术。通常，常见的隐私保护方法可能在数据聚合或处理后应用某种隐私保护技术，而"ε-局部差分隐私"更早地在个体级别引入随机性或噪声，以确保用户的隐私得到更加严格的保护。这种方法的目标是在数据处理的早期阶段就保护隐私，而不是依赖于后期的隐私保护措施。

[[CAIE_A review of applications in federated learning_deposit.pdf#page=23&selection=28,37,28,81|CAIE_A review of applications in federated learning_deposit, page 23]]

> Ilias and Georgios (2019) also added homomorphic encryption to a more robust FL framework, which make it possible to compute aggregation on encrypted client

[[CAIE_A review of applications in federated learning_deposit.pdf#page=24&selection=12,10,14,6|CAIE_A review of applications in federated learning_deposit, page 24]]

>  Lee et al., (2018) make use of LSH to detect similar patients in federated settings.

局部敏感哈希（LSH）也是保持机密性的一种普遍方法（Gionis等人，1999年）。所有特征将通过p-稳定哈希函数映射为加密形式。这种加密模式的主要优势是，哈希表示后两个样本之间的相似性将被保留。这对于数据分析和隐私保护非常重要。这使得可以在保持隐私的同时进行一些数据关联或相似性比较的操作。

[[CAIE_A review of applications in federated learning_deposit.pdf#page=24&selection=26,35,28,31|CAIE_A review of applications in federated learning_deposit, page 24]]

##### **5.4.3.7 Secure aggregation**
secure multiparty computation

这种协议的目标是允许多个参与方在不泄露各自私密数据的情况下对数据进行聚合处理，从而保护数据隐私和安全。

>  Haoa et al., (2019) envisioned a more efficient privacy-preserving scheme for FL, which integrate differential privacy and lightweight homomorphic encryption technology. This protocol, mainly for stochastic gradient descent approach, is robust to curious-but-honest server and collusion between the cloud and server.

差分隐私用于随机化数据以保护隐私，而轻量级同态加密技术允许在加密状态下执行一些计算。

[[CAIE_A review of applications in federated learning_deposit.pdf#page=25&selection=13,37,18,48|CAIE_A review of applications in federated learning_deposit, page 25]]

VerifyNet协议具有独特的能力，可以验证从云端返回的机器学习模型的正确性。这对于确保云端计算结果的准确性和可信度非常重要，尤其是在联邦学习等分布式计算环境中。通过VerifyNet协议，可以增强对模型返回结果的信任，有助于提高云端计算的安全性和可靠性。

> The up-to-date approach proposed by Chen et al., (2020) also concentrated on secure aggregation scheme. They add an extra public parameter dispatch to each client to force them training in a same way, thus detect malicious client easily when making an aggregation stage.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=25&selection=26,48,29,69|CAIE_A review of applications in federated learning_deposit, page 25]]

##### **5.4.3.8 Protection method for FL framework**
MPC（多方计算，Secure Multi-Party Computation）是一种密码学协议和隐私保护技术，用于多个参与方之间在不暴露各自私密输入的情况下进行计算。MPC的主要目标是允许多个参与方共同执行计算任务，而不需要他们透露他们的输入数据给其他参与方或第三方。

MPC的关键特点和目标包括：

1. **隐私保护：** MPC旨在确保每个参与方的输入数据保持隐私，其他参与方无法获得对输入数据的详细了解。
    
2. **计算结果：** 尽管各方的输入数据是私密的，但MPC允许他们共同计算出某个函数的结果，而不泄露输入数据。
    
3. **安全性：** MPC协议提供了数学和密码学保障，确保即使一部分参与方是恶意的，也不会泄漏私密信息或破坏计算结果的正确性。

> To reduce noise, the Hybrid-One scheme combine the use of DP with MPC without compromising accuracy rate, which protect communication messages rely on MPC thus introduce less noise than traditional local DP (Truex et al.,2019)

混合型一方案（Hybrid-One scheme）通过将差分隐私（DP）与多方计算（MPC）相结合，来降低在隐私保护过程中引入的噪声。与传统的本地差分隐私相比，这种组合方式有助于保护通信消息的隐私，同时不会牺牲准确性。这是一种旨在在隐私和数据准确性之间找到平衡的方法，特别适用于需要同时保护数据隐私和维持高准确性的场景。

[[CAIE_A review of applications in federated learning_deposit.pdf#page=26&selection=7,0,9,71|CAIE_A review of applications in federated learning_deposit, page 26]]

> Then the efficient HybridAlpha emerged at the right moment, which combined functional encryption with SMC protocol to achieve the highly-performance model without privacy sacrifice (Xu et al.,2019).

HybridAlpha方案，它是一种高效的隐私保护方法。该方案结合了功能性加密（functional encryption）和SMC（多方计算）协议，以实现在保护隐私的同时获得高性能的机器学习模型。这种方法的出现意味着可以在不牺牲数据隐私的情况下实现高效的计算和模型训练，这对于需要在隐私敏感环境中进行数据分析和机器学习任务的应用非常有价值。

[[CAIE_A review of applications in federated learning_deposit.pdf#page=26&selection=11,32,13,68|CAIE_A review of applications in federated learning_deposit, page 26]]

> Liu, Li, Smith and Sekar (2019) established relationship between FL and sketching algorithm to strength confidentiality.

草图算法是一种不需要存储原始数据身份信息的数据处理技术，这与联邦学习的隐私保护目标相吻合。由于草图算法的特性，它可以帮助提高联邦学习中的数据保密性，同时减少了对原始数据身份的追踪需求。这种关联为在联邦学习中使用草图算法提供了新的可能性，有助于更好地平衡隐私和数据分析的需求。

[[CAIE_A review of applications in federated learning_deposit.pdf#page=26&selection=15,79,17,28|CAIE_A review of applications in federated learning_deposit, page 26]]

trusted execution environments (TEEs)安全飞地

> supporting infrastructure and processes to connect attested bina- ries to specific privacy properties is still immature. [Section 4.2.1]

[[Advances and Open Problems in Federated Learning.pdf#page=60&selection=71,30,72,69|Advances and Open Problems in Federated Learning, page 60]]

> Techniques should be composed to enable Privacy in Depth, with privacy expectations degrading gracefully even if one technique/component of the system is compromised. [Section 4.1]

[[Advances and Open Problems in Federated Learning.pdf#page=60&selection=76,0,80,86|Advances and Open Problems in Federated Learning, page 60]]

> Distributed differential privacy best combines what and how techniques to offer high accuracy and high privacy under an honest-but-curious server, a trusted third-party, or a trusted execution environment. [Sections 4.2.2, 4.4.3]

[[Advances and Open Problems in Federated Learning.pdf#page=61&selection=2,0,14,36|Advances and Open Problems in Federated Learning, page 61]]

> Verifiability enables parties to prove that they have executed their parts of a computation faithfully.

[[Advances and Open Problems in Federated Learning.pdf#page=61&selection=17,0,19,89|Advances and Open Problems in Federated Learning, page 61]]

> Tension may exist when trying to simultaneously improve robustness and privacy in machine learning (Section 5.3).

[[Advances and Open Problems in Federated Learning.pdf#page=73&selection=56,2,57,14|Advances and Open Problems in Federated Learning, page 73]]

#### **5.4.3.9 Collaborative training solutions**

![[Pasted image 20231030205220.png]]

> The key idea of this technique is that instead of uploading the whole set of trained parameters to the server and updating the whole global parameters to its local model, each participant wisely selects the number of gradients to upload and the number of parameters from the global model to updat

[[Federated Learning in Mobile Edge Networks.pdf#page=20&selection=107,15,112,8|Federated Learning in Mobile Edge Networks, page 20]]

### **5.5 Ensuring Fairness and Addressing Sources of Bias**
#### **5.5.1 Bias in training data**
> Bias in training data (Section 6.1) is a key consideration related to bias and fairness in FL models, particularly due to the additional sampling steps germane to federation (e.g., client sampling) and the transfer of some model computation to client devices.

[[Advances and Open Problems in Federated Learning.pdf#page=79&selection=53,3,55,53|Advances and Open Problems in Federated Learning, page 79]]

#### **5.5.2 The lack of data regarding sensitive attributes**
> The lack of data regarding sensitive attributes in many FL deployments can pose challenges for mea- suring and ensuring fairness, and also suggests potential reframing of fairness problems in ways that do not require such data (Section 6.2).

[[Advances and Open Problems in Federated Learning.pdf#page=79&selection=56,3,58,39|Advances and Open Problems in Federated Learning, page 79]]

#### **5.5.3 Tensions between privacy and fairness objectives**
> Since FL is often deployed in contexts which are both privacy- and fairness-sensitive, this can magnify tensions between privacy and fairness objectives in practice.

[[Advances and Open Problems in Federated Learning.pdf#page=79&selection=59,3,60,61|Advances and Open Problems in Federated Learning, page 79]]

#### **5.5.4 Leveraging Federation to Improve Model Diversity**
> Federated learning presents unique opportunities to improve the diversity of stakeholders and data incorporated into learning, which could improve both the overall quality of downstream models, as well as their fairness due to more representative datasets (Section 6.4).

[[Advances and Open Problems in Federated Learning.pdf#page=80&selection=2,2,4,73|Advances and Open Problems in Federated Learning, page 80]]

#### **5.5.5 Federated Fairness: New Opportunities and Challenges**
> Federated learning presents fairness-related challenges not present in the centralized training regime, but also affords new solutions (Section 6.5).

[[Advances and Open Problems in Federated Learning.pdf#page=80&selection=5,3,6,45|Advances and Open Problems in Federated Learning, page 80]]

### **5.6 Addressing System Challenges**
#### **5.6.1 Platform Development and Deployment Challenges**
> Frequent and large scale deployment of updates, monitoring, and debugging is challenging (Sec- tion 7.1).

[[Advances and Open Problems in Federated Learning.pdf#page=88&selection=15,2,16,10|Advances and Open Problems in Federated Learning, page 88]]

#### **5.6.2 System Induced Bias**
> Differences in device availability induce various forms of bias; defining, quantifying and mitigating them remains a direction for future research (Section 7.2).

[[Advances and Open Problems in Federated Learning.pdf#page=88&selection=17,2,18,59|Advances and Open Problems in Federated Learning, page 88]]

#### **5.6.3 System Parameter Tuning**
> Tuning system parameters is difficult due to the existence of multiple, potentially conflicting objec- tives (Section 7.3).

[[Advances and Open Problems in Federated Learning.pdf#page=88&selection=19,3,20,20|Advances and Open Problems in Federated Learning, page 88]]

#### **5.6.4 On-Device Runtime**
> Running ML workloads on end user devices is hampered by the lack of a portable, fast, small footprint, and flexible runtime for on-device training (Section 7.4).

[[Advances and Open Problems in Federated Learning.pdf#page=88&selection=21,2,22,57|Advances and Open Problems in Federated Learning, page 88]]

#### **5.6.5 The Cross-Silo Setting**
> Systems for cross-silo settings (Section 7.5) face largely different issues owing to differences in the capabilities of compute nodes and the nature of the data being processed.

[[Advances and Open Problems in Federated Learning.pdf#page=88&selection=23,0,24,73|Advances and Open Problems in Federated Learning, page 88]]

## **6.应用**

### **6.1 Application for mobile devices**
predict users’ input

Further improvement for prediction on keyboard

emoji prediction

predict human trajectory (Feng et al., 2020) or human behavior (Sozinov et al., 2018)

> combination of FL and MEC, Wang, X. et al., (2019) investigate an ‘In-Edge AI’ framework which combine FL based on deep reinforcement learning with MEC system and further optimize resource allocation problem. 
> 深度强化学习的联邦学习用于在分布式边缘计算环境中进行模型训练和优化，以解决资源分配问题。这种结合可以提高计算效率，减少数据传输延迟，并更好地利用边缘计算资源，有助于实现更智能的边缘计算应用。

MEC（移动边缘计算）允许在边缘设备上进行计算和数据处理，以减少延迟和提高性能，但与之相关的数据处理也增加了信息泄漏的风险。

[[CAIE_A review of applications in federated learning_deposit.pdf#page=27&selection=24,36,31,9|CAIE_A review of applications in federated learning_deposit, page 27]]

> Aïvodji et al., (2019) present a sufficient secure federated architecture to build joint models. 

[[CAIE_A review of applications in federated learning_deposit.pdf#page=28&selection=6,0,7,8|CAIE_A review of applications in federated learning_deposit, page 28]]

> Yu et al., (2020) build a federated multi-task learning framework for smart home IOT to automatically learn users’ behavior patterns, which could effectively detect physical hazards.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=28&selection=7,20,11,36|CAIE_A review of applications in federated learning_deposit, page 28]]

> Liu, B. et al., (2020) proposed a data fusion approach based on FL for robots imitation learning in robot networking.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=28&selection=11,50,12,78|CAIE_A review of applications in federated learning_deposit, page 28]]

### **6.2 Application in Industrial Engineering**
> Hu et al., (2018) designed a novel environmental monitoring frame based on federated region learning FRL) for the sake of inconvenient interchangeable monitor data.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=28&selection=25,72,27,72|CAIE_A review of applications in federated learning_deposit, page 28]]

> L is also applied to visual inspection task (Han et al.,2019).It could not only help us solve the problem of lacking defective samples to detect defects in production tasks but also offered privacy guarantees for manufacturers.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=28&selection=29,37,29,85|CAIE_A review of applications in federated learning_deposit, page 28]]

visual inspection task

malicious attacks detection in communication system composed by Unmanned Aerial Vehicles (UAVs)

> With the popularization of electric vehicles, Saputra et al., (2019) designed a federated energy demand prediction method for various charging stations to prevent energy congestion in transmission process

[[CAIE_A review of applications in federated learning_deposit.pdf#page=29&selection=8,67,11,23|CAIE_A review of applications in federated learning_deposit, page 29]]

>  Yang, Zhang, Ye, Li and C.-Z. Xu (2019) leveraged FL to transactions owned by different banks in order to detect credit card fraud efficiently, which is also a significant contribution to financial field.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=29&selection=11,34,13,79|CAIE_A review of applications in federated learning_deposit, page 29]]

>  For text mining, Wang, Y. et al., (2020) exploit an industrial grade federated framework based on Latent Dirichlet Allocation. It has passed the assessment on real data for spam filtering and sentiment analysis.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=29&selection=13,79,16,33|CAIE_A review of applications in federated learning_deposit, page 29]]

### **6.3 Application in HealthCare**
> Kim, et al., (2017) gave an attempt to use tensor factorization models for phenotyping analysis to obtain information concealed in health records without sharing patient- level data.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=30&selection=6,0,8,11|CAIE_A review of applications in federated learning_deposit, page 30]]

>  Huanga et al., (2019) make use of EMRs scattered across hospitals to predict mortality rate for heart disease patients. During training process, there is not any form of data or parameters transmission among hospitals’ databases. 

[[CAIE_A review of applications in federated learning_deposit.pdf#page=30&selection=11,33,19,11|CAIE_A review of applications in federated learning_deposit, page 30]]

NLP based on FL

> Liu, Dligach and Miller (2019) focus on need for unstructured data processing of clinical notes.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=31&selection=5,76,9,6|CAIE_A review of applications in federated learning_deposit, page 31]]

> Federated principal components analysis (fPCA) has been put forward by Silva et al., (2019) to extract features from magnetic resonance images (MRI) come from different medical centers.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=31&selection=15,0,17,26|CAIE_A review of applications in federated learning_deposit, page 31]]

> Gao et al., (2019) proposed a hierarchical heterogeneous horizontal FL (HHHFL) framework for Electroencephalography (EEG) classification to overcome the challenge of limited labeled instances as well as the privacy constraint.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=31&selection=17,40,20,23|CAIE_A review of applications in federated learning_deposit, page 31]]

### **6.4 Mobile edge network optimization**

![[Pasted image 20231030210633.png]]

#### **6.4.1 Cyberattack Detection**
> each edge node operates as a participant who owns a set of data for intrusion detection

[[Federated Learning in Mobile Edge Networks.pdf#page=24&selection=157,53,159,23|Federated Learning in Mobile Edge Networks, page 24]]

> IoT gateways operate as FL participants and an IoT security service provider works as a server node to aggregate trained models shared by the participants

[[Federated Learning in Mobile Edge Networks.pdf#page=24&selection=169,49,172,15|Federated Learning in Mobile Edge Networks, page 24]]

#### **6.4.2 Edge Caching and Computation Offloading**

#### **6.4.3 Base station assoication **

#### **6.4.4 Vehicular networks **


## **7.未来方向**

### **7.1 Extreme communication schemes**

### **7.2 Communication reduction and the Pareto frontier**

Pareto frontier

### **7.3 Novel models of asynchrony**

device-centric communication scheme

### **7.4 Heterogeneity diagnostics**

 (i) Do simple diagnostics exist to quickly determine the level of heterogeneity in federated networks a priori?
 (ii) Can analogous diagnostics be developed to quantify the amount of systems-related heterogeneity? 
 (iii) Can current or new definitions of heterogeneity be exploited to further improve the convergence of federated optimization methods?

### **7.5 Granular privacy constraints**

Developing methods to handle mixed (device-specific or sample-specific) privacy restrictions

### **7.5 Beyond supervised learning**

scalability, heterogeneity, and privacy

### **7.6 Productionizing federated learning**

concept drift(when the underlying data-generation model changes over time)
diurnal variations(when the devices exhibit different behavior at different times of the day or week)
cold start problems(when new devices enter the network)

### **7.7 Benchmarks**

### **7.8 Security compliance establishment**

### **7.9 Attack defense and efficiency promotion**

### **7.10 Asynchronous training mode**
> the synchronous training has already become the major form for FL due to superior performance of SGD in the central server settings compared to asynchronous way (Chen, Ning and Rangwala, 2019; Mohammad and Sorour, 2019) Prior optimization of FL mainly

[[CAIE_A review of applications in federated learning_deposit.pdf#page=32&selection=43,62,49,74|CAIE_A review of applications in federated learning_deposit, page 32]]

>  asynchronous online FL framework presented by Chen et al., (2019) updates central model in an asynchronous way by introducing feature learning and dynamic learning step size.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=33&selection=22,29,26,40|CAIE_A review of applications in federated learning_deposit, page 33]]

> Wu et al., (2019) proposed a semi- asynchronous protocol which allow straggling clients don’t always go together with central server.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=33&selection=28,46,32,15|CAIE_A review of applications in federated learning_deposit, page 33]]

### **7.11 Gradient aggregation**
> Yao et al., (2019) keep trace of dispatched global parameters in each local training epoch. Since local gradient update is a function of global parameters, then gradients can be aggregated in an unbiased way. 

[[CAIE_A review of applications in federated learning_deposit.pdf#page=34&selection=8,75,14,32|CAIE_A review of applications in federated learning_deposit, page 34]]

>   Ji et al., (2019) introduce a recurrent neural network aggregator to automatically get an optimized way for gradient aggregation. 

[[CAIE_A review of applications in federated learning_deposit.pdf#page=34&selection=14,73,18,40|CAIE_A review of applications in federated learning_deposit, page 34]]

> Wang et al., (2019) designed a layer-wise aggregation mode to serially generate layer parameters in neural network for global model. 

[[CAIE_A review of applications in federated learning_deposit.pdf#page=34&selection=18,53,25,18|CAIE_A review of applications in federated learning_deposit, page 34]]

### **7.11 Incentive mechanism**
> Sarikaya and Ercetin (2019) explore inventive mechanism in Stackelberg perspective to inspire workers to allocate more CPU for local training

[[CAIE_A review of applications in federated learning_deposit.pdf#page=35&selection=2,30,6,14|CAIE_A review of applications in federated learning_deposit, page 35]]

>  Khan et al., (2019) discussed Stackelberg-based incentive mechanism to set local iteration times adaptively to be effective as much as possible.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=35&selection=6,14,8,76|CAIE_A review of applications in federated learning_deposit, page 35]]

### **7.12 Verification for returned model**
>   Li  et al., (2019) considered an autoencoder enable model parameters to be replaced by low-dimension vector as well as discover irregular weights update

[[CAIE_A review of applications in federated learning_deposit.pdf#page=35&selection=39,41,43,35|CAIE_A review of applications in federated learning_deposit, page 35]]

>   Muñoz-González and Lupu (2019) discussed adaptive FL to grub abnormal updates via a Hidden Markov Model to evaluate model quality.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=35&selection=43,35,47,8|CAIE_A review of applications in federated learning_deposit, page 35]]

### **7.13 FL with block-chain technology**
> Hence Ilias and Georgios (2019) utilized blockchain smart convention to coordinate all clients and additionally used homomorphic encryption to provide extra privacy guarantee.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=36&selection=46,66,50,76|CAIE_A review of applications in federated learning_deposit, page 36]]

> The blockchain-based privacy-preserving FL framework designed by Awan et al., (2019) also added a variation of the Paillier cryptosystem as an excess measure to forestall privacy leakage. 

[[CAIE_A review of applications in federated learning_deposit.pdf#page=37&selection=0,0,2,17|CAIE_A review of applications in federated learning_deposit, page 37]]

### **7.14 Dropped participants**

### **7.15 Unlabeled data**

### **7.16 Interference among mobile devices**

> These mobile devices may be geographically close to each other, i.e., in the same cell. This introduces an interference issue when they update local models to the server

[[Federated Learning in Mobile Edge Networks.pdf#page=28&selection=64,26,67,26|Federated Learning in Mobile Edge Networks, page 28]]

### **7.17 Cooperative mobile crowd ML**

> mobile devices nearby can be grouped in a cluster, and the model downloading/uploading between the server and the mobile devices can be facilitated by a “cluster head” that serves as a relay nod

[[Federated Learning in Mobile Edge Networks.pdf#page=29&selection=97,15,110,46|Federated Learning in Mobile Edge Networks, page 29]]



## 经验风险 Empirical risk
#以后可以延伸的知识/经验风险
[[Federated Learning Challenges, Methods, and Future Directions.pdf#page=3&selection=91,51,91,66|Federated Learning Challenges, Methods, and Future Directions, page 3]]
经验风险，基于训练集所有样本点损失函数的平均最小化。经验风险是**局部最优**，是**现实**的可求的。

******经验风险=经验损失=代价函数******

给定一个数据集，模型f(x)关于训练集的平均损失被称为经验风险(empirical risk)或经验损失(empirical loss)。

![](https://img2020.cnblogs.com/blog/968024/202009/968024-20200911200524332-1536630142.png)

这个公式的用意很明显，就是模型关于训练集的平均损失（每个样本的损失加起来，然后平均一下）。在实际中用的时候，我们也就很自然的这么用了。

### 经验风险最小化

**经验风险最小化（****empirical risk minimization，****ERM），**就是认为经验风险最小的模型是最优的模型，用公式表示：

![](https://img2020.cnblogs.com/blog/968024/202009/968024-20200911200938304-535333836.png)  
这个理论很符合人的直观理解。因为在训练集上面的经验风险最小，也就是平均损失越小，意味着模型得到结果和“真实值”尽可能接近，表明模型越好。

### ****经验风险最小化的例子：****极大似然估计（maximum likelihood estimation）。

模型，条件概率分布；

损失函数，对数损失函数；

经验风险最小化等价于极大似然估计。


## Model updates as part of the training process
#以后可以延伸的知识 
[[Federated Learning Challenges, Methods, and Future Directions.pdf#page=3&selection=213,1,216,31|Federated Learning Challenges, Methods, and Future Directions, page 3]]

## Stragglers
#以后可以延伸的知识/stragglers
[Straggling Workers in Distributed Computing | by Benjamin | The Startup | Medium](https://medium.com/swlh/straggling-workers-in-distributed-computing-ec60776c7a73)

### 背景知识
#### Clusters
A cluster, in this realm, is a collection of dedicated computational hardware that can communicate; usually over a dedicated private network (but keep in mind distributed systems exist over public networks).

**Single end-point nodes of this cluster** are what we call **workers **

#### stragglers的含义
In cloud computing and high performance computing, a large job is typically divided into many small tasks for parallel execution in a distributed environment. Due to different reasons, some tasks (so-called **stragglers**) are considerably slower than the others, delaying the completion of the job.

### Meta-Learning
#以后可以延伸的知识/元学习 
[一文入门元学习（Meta-Learning）（附代码） - 知乎](https://zhuanlan.zhihu.com/p/136975128)
元学习Meta Learning，含义为学会学习，即learn to learn，就是带着这种对人类这种“学习能力”的期望诞生的。Meta Learning希望使得模型获取一种“学会学习”的能力，使其可以在获取已有“知识”的基础上快速学习新的任务，如：

- 让Alphago迅速学会下象棋
- 让一个猫咪图片分类器，迅速具有分类其他物体的能力

**需要注意的是，虽然同样有“预训练”的意思在里面，但是元学习的内核区别于迁移学习（Transfer Learning）**

![[Pasted image 20230917140008.png]]

在机器学习中，**训练单位是一条数据**，通过数据来对模型进行优化；数据可以分为训练集、测试集和验证集。在元学习中，训练单位分层级了，**第一层训练单位是任务，也就是说，元学习中要准备许多任务来进行学习，第二层训练单位才是每个任务对应的数据**。

二者的目的都是找一个Function，只是两个Function的功能不同，要做的事情不一样。机器学习中的Function直接作用于特征和标签，去寻找特征与标签之间的关联；而元学习中的Function是用于寻找新的f，新的f才会应用于具体的任务。

### Multi-task learning
#以后可以延伸的知识/多任务学习 

### Data silo
#以后可以延伸的知识/数据筒仓 
[What is a Data Silo? | TIBCO Software](https://www.tibco.com/reference-center/what-is-a-data-silo)
A **data silo** is a collection of information isolated from an organization and inaccessible to all parts of a company hierarchy.

### Logistic regression
#以后可以延伸的知识/逻辑回归
[Module 4 - Logistic Regression | The Programming Foundation](https://learn.theprogrammingfoundation.org/getting_started/intro_data_science/module4/?gclid=Cj0KCQjw06-oBhC6ARIsAGuzdw2CzvjvLPVT9PNkKsYMbsat4oXbGZhbsaSNrbxh5h1SFp5HjFqc7y4aAiY0EALw_wcB)
#### **What is Logistic Regression?**

The logistic regression statistic modeling technique is used when we have a binary outcome variable. For example: given the parameters, will the student pass or fail? Will it rain or not? etc.

### Homomorphic Encryption
#以后可以延伸的知识/同态加密
[What Is Homomorphic Encryption & How Is It Used | Venafi](https://venafi.com/blog/homomorphic-encryption-what-it-and-how-it-used/)
The [purpose of homomorphic encryption](https://eprint.iacr.org/2015/1192.pdf) is to allow computation on encrypted data.It apart from other forms of encryption is that it uses an algebraic system to allow a variety of computations (or operations) on the encrypted data.

In mathematics, [homomorphic](https://searchsecurity.techtarget.com/definition/homomorphic-encryption) describes the transformation of one data set into another while preserving relationships between elements in both sets.

most homomorphic encryption schemes work best with data represented as integers and while using addition and multiplication as the operational functions.

#### **Why use homomorphic encryption**

Organizations can use traditional encryption methods to secure sensitive data on cloud environments. But if they need to investigate or validate encrypted data in the cloud, they would need to either decrypt the data or download it and decrypt it. The first option can lead to security problems and the second can be costly and time-consuming.

#### **Types of Homomorphic Encryption**

There are three types of homomorphic encryption. The primary difference between them is related to the types and frequency of mathematical operations that can be performed on the ciphertext. The three types are:

1.Partially Homomorphic Encryption
	 Only select mathematical functions to be performed on encrypted values. This means that only one operation, either addition or multiplication, can be performed an unlimited number of times on the ciphertext.PHE with multiplicative operations is the foundation for RSA encryption, which is commonly used in establishing secure connections through SSL/TLS.
2.Somewhat Homomorphic Encryption
	A somewhat homomorphic encryption (SHE) scheme is one that supports select operation (either addition or multiplication) up to a certain complexity, but these operations can only be performed a set number of times.
3.Fully Homomorphic Encryption
	Fully homomorphic encryption (FHE) [has a lot of potential](https://www.thesslstore.com/blog/what-is-homomorphic-encryption/) for making functionality consistent with privacy by helping to keep information secure and accessible at the same time.

### Security model
#以后可以延伸的知识/安全模型
[Security model - MPC wiki](https://wiki.mpcalliance.org/security_model.html)
we should define the security goal, which is called the security model in cryptography. The semi-honest model and malicious model are widely considered to capture the capabilities of adversaries.

#### **1.Semi-Honest Adversary**

In the semi-honest setting, the players strictly follow the instructions of the protocol, and are still curious to learn information of other inputs from the transactions. This is meaningful in some enterprise-to-enterprise business scenarios, where the enterprises must strictly follow the procedure and the codes are run in some safe place that stuffs can not touch.

#### **2.Malicious Adversary**

The malicious setting (or active security) is more realistic than the semi-honest setting. In the malicious setting players can do what ever they want to obtain private information of other parties inputs. This means they can arbitrarily deviate from the protocol in any way they feel.

The malicious setting comes in two variants. In the first variant called active-security-with-abort the honest players will abort the protocol when the adversary deviates from the protocol, or more realistically the honest players abort with overwhelming probability if the adversary deviates. In the second variant, called robust security, the honest players are able to continue with the protocol even though the malicious players have deviated from the protocol.

Active-security-with-abort can be obtained no matter how many parties one has, and no matter how many adversaries. However, the second variant of robust security can only be obtained when one has an honest majority; i.e. the number of honest parties is more than the number of dishonest parties. Thus in the important case of two party multi-party computation one can only obtain active-security-with-abort.

### Stochastic gradient descent
#以后可以延伸的知识/随机梯度下降

### Cross-entropy loss
#以后可以延伸的知识/交叉熵损失
[Site Unreachable](https://machinelearningmastery.com/cross-entropy-for-machine-learning/)
Cross-entropy is a measure from the field of information theory, building upon [entropy](https://machinelearningmastery.com/what-is-information-entropy/) and generally calculating the difference between two probability distributions.It is closely related to but is different from [KL divergence](https://machinelearningmastery.com/divergence-between-probability-distributions/) that calculates the relative entropy between two probability distributions, whereas cross-entropy can be thought to calculate the total entropy between the distributions.

Cross-entropy is also related to and often confused with [logistic loss, called log loss](https://machinelearningmastery.com/logistic-regression-with-maximum-likelihood-estimation/). Although the two measures are derived from a different source, when used as loss functions for classification models, both measures calculate the same quantity and can be used interchangeably.

#### **1.What Is Cross-Entropy?**

Cross-entropy is a measure of the difference between two probability distributions for a given random variable or set of events.Lower probability events have more information, higher probability events have less information.

Information _h(x)_ can be calculated for an event _x_, given the probability of the event _P(x)_ as follows:

- h(x) = -log(P(x))

Entropy _H(x)_ can be calculated for a random variable with a set of _x_ in _X_ discrete states discrete states and their probability _P(x)_ as follows:

- H(X) = – sum x in X P(x) * log(P(x))

The intuition for this definition comes if we consider a target or underlying probability distribution P and an approximation of the target distribution Q, then the cross-entropy of Q from P is the number of additional bits to represent an event using Q instead of P.

The cross-entropy between two probability distributions, such as Q from P, can be stated formally as:

- H(P, Q)

Where H() is the cross-entropy function, P may be the target distribution and Q is the approximation of the target distribution.

Cross-entropy can be calculated using the probabilities of the events from P and Q, as follows:

- H(P, Q) = – sum x in X P(x) * log(Q(x))

Where P(x) is the probability of the event x in P, Q(x) is the probability of event x in Q and log is the base-2 logarithm, meaning that the results are in bits. If the base-e or natural logarithm is used instead, the result will have the units called nats.

This calculation is for discrete probability distributions, although a similar calculation can be used for continuous probability distributions using the integral across the events instead of the sum.

The result will be a positive number measured in bits and will be equal to the entropy of the distribution if the two probability distributions are identical.

说实话我没看懂，还是看这个吧[機器/深度學習: 基礎介紹-損失函數(loss function) | by Tommy Huang | Medium](https://chih-sheng-huang821.medium.com/%E6%A9%9F%E5%99%A8-%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-%E5%9F%BA%E7%A4%8E%E4%BB%8B%E7%B4%B9-%E6%90%8D%E5%A4%B1%E5%87%BD%E6%95%B8-loss-function-2dcac5ebb6cb)

### Data poisoning
#以后可以延伸的知识/数据污染
当涉及到数据污染攻击时，可以进一步解释这两种主要类型的攻击：

1. 模型偏差（Model Skew）攻击：
   - 模型偏差攻击旨在通过操纵训练数据来引导机器学习模型产生错误的预测或决策。
   - 攻击者可能会有意修改训练数据中的一些样本，以便模型在未来的预测中偏向攻击者所期望的结果。
   - 举例来说，如果攻击者想要迷惑一个垃圾邮件过滤器，他们可以在训练数据中添加一些正常邮件的特征，使垃圾邮件模型更容易将正常邮件误分类为垃圾邮件。

2. 反馈武器化（Feedback Weaponization）攻击：
   - 反馈武器化攻击是一种更加复杂和有针对性的攻击方式，它利用模型的输出反馈来进一步损害模型的性能或达到攻击者的目标。
   - 攻击者可能会监视模型的输出，然后有针对性地选择性地修改输入数据，以使模型的输出满足他们的意图。
   - 例如，如果攻击者针对一个电子商务推荐系统，他们可能会观察系统的建议并采取行动来引导系统向某种产品或服务提供更多的推荐，以增加其销售量或影响用户决策。

这些攻击类型都旨在破坏机器学习模型的可靠性和安全性，可能导致模型在真实世界中的应用中产生不良影响。因此，研究和开发防御数据污染攻击的方法变得至关重要，特别是在联邦学习等分布式学习环境中，模型基于多个数据源进行训练。

### Sketching algorithm
#以后可以延伸的知识/草图算法
草图算法（Sketching Algorithm）是一种用于数据降维、压缩和摘要的计算技术。它的主要目标是将大规模数据集转化为相对较小的表示形式，同时尽量保留数据的关键信息。草图算法在数据挖掘、数据压缩、近似查询和分布式计算等领域中得到广泛应用。

草图算法的特点包括：

1. **降维：** 草图算法可以将高维数据映射到低维空间，从而减小数据的维度。这有助于降低计算和存储成本。

2. **压缩：** 草图算法可以将原始数据进行压缩，减小数据的体积，同时保留数据的某些特征或结构。

3. **近似：** 草图算法提供了对原始数据的近似表示，允许在不处理完整数据集的情况下进行某些查询或分析操作。

4. **摘要：** 草图算法生成了数据的摘要，用于快速汇总数据的信息。

草图算法在大数据处理、数据挖掘、数据库查询优化和网络流量分析等方面发挥了重要作用。它们通常用于解决数据量庞大的问题，以提高计算和查询效率，同时在保留数据关键特征的基础上实现数据的压缩和摘要。

### Federated Principal Components Analysis
#以后可以延伸的知识/联邦主成分分析 
PCA（Principal Component Analysis，主成分分析）是一种常用的数据降维和特征提取技术，广泛应用于数据分析、模式识别和机器学习领域。PCA的主要目标是通过线性变换将高维数据转换为低维数据，同时保留数据中的最重要信息。这种变换通常通过找到数据的主成分来实现，这些主成分是原始数据中方差最大的线性组合。

PCA的工作原理如下：

1. **数据中心化：** 首先，对原始数据进行中心化处理，即减去数据的均值，以确保数据的均值为零。

2. **计算协方差矩阵：** 然后，计算中心化数据的协方差矩阵。协方差矩阵描述了不同特征之间的关联性和方差。

3. **特征值分解：** 对协方差矩阵进行特征值分解，得到特征值和对应的特征向量。特征向量是协方差矩阵的特征结构，表示了数据中的主要方向。

4. **选择主成分：** 根据特征值的大小，选择前k个特征向量作为主成分，其中k是降维后的维度。通常，选择的特征向量对应于最大的特征值，因为它们包含了最多的信息。

5. **投影数据：** 将原始数据投影到所选的主成分上，得到降维后的数据。

PCA的主要应用包括数据可视化、噪声去除、特征选择、数据压缩等。通过降维，PCA可以减少数据的维度，减小计算复杂度，同时保留数据中的关键信息，有助于提高模型的性能和效率。

Federated Principal Components Analysis（联邦主成分分析）是一种联邦学习（Federated Learning）的应用，用于分析和降维多个分布式数据集的方法。主成分分析（PCA）是一种常用的降维技术，用于减少数据维度并保留数据集中的关键信息。在联邦学习背景下，Federated PCA允许多个参与方协作执行PCA，而无需共享原始数据。

以下是Federated PCA的主要特点和工作原理：

1. **分布式数据：** 在联邦学习中，数据存储在多个分布式参与方的本地，不需要将数据集中到中心服务器。每个参与方都有自己的数据集。

2. **隐私保护：** Federated PCA确保了数据隐私，因为原始数据不会离开参与方的本地。模型参数和中间结果是唯一共享的信息。

3. **局部PCA：** 每个参与方执行PCA分析其本地数据，得到局部主成分。这样可以在保护隐私的同时，分析每个参与方的数据特点。

4. **聚合结果：** 参与方将局部主成分合并或聚合起来，以获得全局主成分。这些全局主成分用于描述整个数据集的结构和变化。

5. **降维和特征提取：** Federated PCA通常用于数据降维和特征提取任务。它可以帮助减少数据维度，去除冗余信息，并提取最重要的特征。

Federated PCA的应用领域包括医疗保健、金融、物联网等需要分析多个分布式数据源的领域。这种方法允许数据所有者在保护隐私的同时，合作进行数据分析和模型建设。

### Dynamic learning step size
#以后可以延伸的知识/动态学习步长
动态学习步长（Dynamic Learning Rate）是机器学习和深度学习中的一个重要概念。学习步长（Learning Rate）是一个控制模型参数更新幅度的超参数，它决定了模型在每次迭代中应该学习多少。动态学习步长是指在训练过程中根据模型的性能和训练进展动态地调整学习步长的技术。

通常情况下，学习步长是一个常数，例如0.01或0.001，它在整个训练过程中保持不变。但在某些情况下，特别是在深度神经网络的训练中，使用动态学习步长可以带来以下好处：

1. **加速收敛：** 在训练初期，模型可能需要较大的学习步长以加快收敛速度。随着训练的进行，学习步长可以逐渐减小，以更精细地调整模型参数。

2. **避免震荡：** 如果学习步长设置得太大，模型的参数可能会在最优值附近震荡，而不是稳定地收敛。动态学习步长可以减小这种震荡的可能性。

3. **自适应性：** 动态学习步长可以根据模型在每次迭代中的性能自适应地调整。如果模型的性能改善较慢，学习步长可以减小，以更小的步伐前进，反之亦然。

动态学习步长的调整可以采用各种方法，包括指数衰减、自适应方法（如Adam优化器中的自适应学习步长）和基于性能的调整。这些方法的目标是确保模型在训练过程中更有效地收敛到最佳解决方案，同时避免梯度下降中的一些问题，如梯度爆炸或梯度消失。

### Gradient Descent
#以后可以延伸的知识/梯度下降
在梯度下降（Gradient Descent）等优化算法中，可能会出现一些问题，这些问题可能会影响模型的训练效果和收敛性能。以下是一些常见的梯度下降中的问题：

1. **梯度消失（Gradient Vanishing）：** 当深度神经网络具有很多层时，梯度可能会在反向传播过程中逐渐减小到接近零，导致底层的参数几乎不会更新。这会导致底层的模型学习得非常缓慢，甚至根本无法收敛。

2. **梯度爆炸（Gradient Exploding）：** 与梯度消失相反，梯度在反向传播中可能会增长得非常快，导致参数更新的幅度过大。这会导致模型参数发散，无法获得有效的训练。

3. **局部最优解（Local Optima）：** 梯度下降可能会陷入局部最优解，而无法找到全局最优解。这在高维空间中特别常见，因为存在许多局部最优解。

4. **学习率选择（Learning Rate Selection）：** 选择不合适的学习率可能会导致训练不稳定或者收敛速度过慢。学习率太大可能导致模型不稳定，学习率太小可能导致收敛速度非常慢。

5. **过拟合（Overfitting）：** 如果梯度下降训练的轮次过多，模型可能会过度拟合训练数据，导致在新数据上的泛化性能下降。

6. **初始权重选择（Initialization）：** 初始权重的选择可能会对梯度下降的性能产生重要影响。糟糕的初始化可能导致模型陷入不稳定的状态。

为了应对这些问题，研究人员已经提出了各种改进和优化的梯度下降算法，包括随机梯度下降（SGD）、带动量的梯度下降（Momentum）、自适应学习率的方法（如Adam、RMSProp等）以及正则化技术等。这些方法旨在提高梯度下降的稳定性和效率，从而更好地训练深度学习模型。

### Stackelberg
#以后可以延伸的知识/Stackelberg
"Stackelberg" 是博弈论中的一个重要概念，用于描述一种博弈模型，其中一方的行动会影响另一方的决策，并且这两方的角色不是对等的。Stackelberg博弈是一种领导者-追随者博弈，其中有一个领导者（Leader）和一个或多个追随者（Follower）。

在Stackelberg博弈中，领导者首先做出决策，而追随者在看到领导者的决策后做出自己的决策。领导者的目标是最大化自己的利益，而追随者的目标是在领导者的决策下实现自己的最佳利益。

Stackelberg博弈的特点包括：

1. **信息不对称（Asymmetric Information）：** 领导者拥有更多的信息，他们知道追随者的反应将根据领导者的决策而变化。

2. **顺序性（Sequential）：** 领导者首先行动，然后追随者作出响应。这种顺序性是Stackelberg博弈的关键特征。

3. **领导者优势（Leader's Advantage）：** 领导者在制定决策时可以考虑追随者的反应，从而有利于他们更好地优化自己的利益。

4. **策略互动（Strategic Interaction）：** 领导者和追随者之间的策略互动是Stackelberg博弈的核心，领导者的决策会影响追随者的最佳响应。

Stackelberg博弈在经济学、管理学、工程学和其他领域中都有广泛的应用。它通常用于描述市场中的价格领导者和价格追随者之间的动态互动，也可以用于分析博弈理论中的策略性决策问题。

### Crowdsourcing
#以后可以延伸的知识/Crowdsourcing
众包（Crowdsourcing）是一种通过向广泛的人群或社区寻求外部协助、创意或劳动力来完成任务、解决问题或获取信息的方法。众包的核心思想是将一个任务或项目分发给大量在线参与者，这些参与者可以是普通公众、志愿者、独立专业人士或特定领域的专家。众包通常通过互联网平台或应用程序进行组织和管理。

众包的关键特点包括：

1. **大规模参与：** 众包利用了大规模在线社区的力量，从而可以吸引成百上千甚至成千上万的参与者，以完成各种任务。

2. **多样性：** 参与者可以具有不同的技能、背景和知识，因此众包可以用于解决各种类型的问题，从简单的数据标注到复杂的科学研究。

3. **成本效益：** 众包通常更具成本效益，因为它可以避免雇佣全职员工或专家，而是按需支付参与者。

4. **加速任务完成：** 通过将任务分发给大量参与者，众包可以加速任务的完成速度，尤其是在需要大量人力的项目中。

5. **多领域应用：** 众包可用于各种领域，包括数据标注、文本翻译、图像识别、科学研究、创意设计等。

6. **问题解决和创新：** 众包不仅用于任务执行，还可以用于创意生成和问题解决。通过众包，可以获得新的观点和创意。

众包在商业、科学研究、非营利组织和政府等各个领域都有广泛的应用。例如，众包可以用于创建地图数据、进行市场研究、改进自然语言处理算法、解决科学难题等。这种方法已经成为利用全球社区的智慧和资源来解决各种问题的有效途径。

###  Hidden Markov Mode
#以后可以延伸的知识/隐马尔可夫模型
隐马尔可夫模型（Hidden Markov Model，HMM）是一种数学统计模型，广泛用于建模具有时序性质的数据，例如语音识别、自然语言处理、生物信息学、金融分析等领域。HMM 的名称中包含了两个关键概念：

1. **马尔可夫模型（Markov Model）：** 马尔可夫模型是一种描述状态变化的数学模型，其中假设系统的未来状态只取决于当前状态，而不受过去状态的影响。这就是所谓的马尔可夫性质。马尔可夫模型通常用状态和状态转移概率来表示，可以用有向图表示状态之间的转移关系。

2. **隐（Hidden）：** 隐马尔可夫模型之所以称为“隐”，是因为它涉及到两组变量：观察变量（Observations）和隐藏状态变量（Hidden States）。观察变量是我们可以观察到或测量到的数据，而隐藏状态变量则是模型内部的状态，通常不可直接观察到，但会影响观察数据的生成过程。

HMM 通常用于以下两个主要问题：

1. **状态估计问题（State Estimation）：** 给定观察序列和模型参数，要求计算最可能的隐藏状态序列。这通常使用前向算法、后向算法或维特比算法来解决。

2. **参数学习问题（Parameter Learning）：** 给定观察序列，要求估计模型的参数，包括状态转移概率、初始状态概率和观察概率分布。这通常使用期望最大化（Expectation-Maximization，EM）算法来解决。

HMM 在自然语言处理中用于词性标注、语音识别中用于语音模型、生物信息学中用于DNA序列分析、金融领域中用于时间序列分析等多个领域都有广泛的应用。它的强大之处在于能够处理具有时序性和潜在隐藏结构的数据，并用概率的方式建模不确定性。

### Wall-clock time
#以后可以延伸的知识/挂钟时间
"Wall-clock time"（挂钟时间）是指从某个事件开始到结束所经过的实际时间，通常以秒、分钟、小时等为单位表示。这是一个用于测量时间流逝的术语，不考虑计算机或系统的性能，而只关注实际经过的时间。在计算中，特别是在分布式系统或计算任务中，了解 wall-clock time 对于评估性能和计划任务非常重要。

### Secure shuffling
#以后可以延伸的知识/数据混洗
"Secure shuffling"是一种安全的数据混洗技术，通常用于隐私保护和数据安全领域。这种技术的目标是对数据进行混洗或重新排列，以使数据的顺序在混洗之后不能被轻易还原或推断出来。这对于处理敏感数据或保护用户隐私非常重要，因为它可以防止恶意用户或攻击者通过分析数据的顺序来获取敏感信息。

Secure shuffling通常涉及使用密码学方法或协议来确保数据混洗的安全性。它可以在分布式环境中用于隐私保护，例如在联邦学习（Federated Learning）中，以确保在不泄露个别用户数据的情况下混洗和聚合模型更新。此外，安全混洗还可以在加密通信中使用，以确保数据的机密性。

总之，secure shuffling是一种保护数据隐私和安全性的技术，通过混洗数据的顺序来减少潜在的风险和威胁。

### Distributed ledger
#以后可以延伸的知识/分布式账本
分布式账本，也被称为区块链（Blockchain），是一种去中心化的数据结构和技术，用于记录和管理交易或信息的公开、不可篡改的账本。它的主要特点包括：

1. **去中心化：** 区块链不依赖于单一的中央机构或中介，而是由网络中的多个节点共同维护和验证账本的完整性。这意味着没有单一的权威掌控账本，而是由分布在网络中的多个节点一起验证交易。

2. **公开透明：** 区块链的账本通常是公开可访问的，任何人都可以查看其中的交易记录。这种透明性有助于防止欺诈和潜在的不当行为。

3. **不可篡改：** 一旦交易被添加到区块链中，几乎不可能更改或删除。这是因为每个区块包含了前一个区块的信息，创建了一个连续的链条，使修改一个区块的信息需要同时修改所有后续区块，这是极其困难的。

4. **安全性：** 区块链使用加密技术来保护数据的安全性。交易需要经过验证，并且只有经过授权的用户才能进行交易。

5. **智能合约：** 区块链技术支持智能合约，这是自动执行的计算机程序，可以在满足特定条件时执行操作。智能合约可以用于自动化各种业务流程。

6. **加密货币：** 区块链技术的最著名用例之一是加密货币，如比特币和以太坊。这些数字货币使用区块链来记录交易和管理货币供应。

7. **多个应用领域：** 区块链技术不仅限于加密货币，还可以应用于供应链管理、投票系统、身份验证、不动产登记、医疗保健、金融服务和更多领域。

总之，分布式账本（区块链）是一种创新的技术，它提供了一种去中心化、透明、不可篡改和安全的方式来记录和管理数据，对于许多不同的应用领域都具有潜在的重要价值。

区块链的"ledger"指的是区块链网络上的分布式总账或账本。这个账本包含了区块链网络上的所有交易记录。每个区块包含一组交易，这些区块按顺序链接在一起，形成一个不断增长的链。这个账本是分布式的，即它存在于整个区块链网络的每个节点上，而不是存储在单一的中央位置。这意味着所有网络参与者都可以查看和验证账本上的交易，从而增强了透明性和安全性。

区块链的ledger是不可篡改的，因为每个区块都包含前一个区块的哈希值，使得修改账本中的一个区块将会导致后续所有区块无效。这种特性使得区块链在加密货币和其他领域中成为一个强大的工具，因为它确保了交易的不可逆性和安全性。

### Smart contracts
#以后可以延伸的知识/智能合约
 当然，智能合约是区块链技术的一个重要方面。它们是自执行合同，合同的条款直接写入代码中。这些合同可以自动促进、验证或执行协议的谈判或履行，确保各方之间的信任和透明性，而无需中介。

以下是关于智能合约的一些关键点：

1. **基于代码的合同：** 智能合约是以代码编写的，通常使用区块链特定的编程语言，如Solidity（用于以太坊）。它们包含预定义的规则和逻辑，当满足特定条件时，可以自动执行操作。

2. **自动化：** 一旦部署到区块链上，智能合约可以在不需要中介的情况下自动执行操作。例如，它们可以在满足某些条件时释放资金，例如达成共识或特定日期。

3. **透明性：** 智能合约存储在区块链上，使其代码和执行历史对所有参与者可见。这种透明性有助于确保遵循合同条款，并减少争议的可能性。

4. **信任：** 智能合约的参与者可以相信合同将按照规定的方式执行，而不需要依赖中央权威。这种信任是通过区块链的分散化和不可变性特性建立的。

5. **不可变性：** 一旦部署，智能合约难以更改。这种不可变性确保合同条款在达成协议后无法更改。

6. **分散化：** 智能合约在分散化的区块链网络上运行，这意味着没有单一实体对合同的执行拥有控制权。

7. **用途广泛：** 智能合约具有广泛的用途，包括供应链管理、金融服务（例如贷款和保险）、投票系统等等。

总的来说，智能合约是一种强大的工具，可用于以无需信任和分散化的方式自动化和保障各种流程和协议。

### Secure enclaves
#以后可以延伸的知识/安全飞地
安全飞地（Secure Enclaves）是一种硬件和软件协同工作的计算环境，旨在提供高度安全和保护的计算环境。它们通常基于特殊的硬件组件，如Intel SGX（Software Guard Extensions）或AMD SEV（Secure Encrypted Virtualization），以及相应的软件支持。主要特点包括：

1. **隔离性**: 安全飞地可以在主机操作系统之上创建一个隔离的计算环境，其中的代码和数据被保护免受外部干扰和访问。这意味着安全飞地内的应用程序可以在不受主机操作系统或其他应用程序影响的情况下运行。

2. **加密**: 安全飞地通常使用硬件加密来保护内部数据。这意味着即使在物理攻击下，攻击者也难以访问或破解内部数据。

3. **完整性验证**: 安全飞地内的代码和数据可以进行完整性验证，以确保它们在执行过程中没有被篡改。

4. **隐私保护**: 安全飞地可以用于处理敏感数据，同时保护数据的隐私。这使得它们在安全多方计算和隐私保护领域特别有用。

5. **透明性**: 对于应用程序开发者来说，安全飞地通常提供了相对透明的编程模型，使他们能够开发安全的应用程序而无需深入了解底层的安全性细节。

安全飞地的应用范围广泛，包括保护加密密钥、执行隐私保护计算、运行安全合同等。这些技术在云计算、区块链、隐私保护、安全多方计算等领域发挥着重要作用。

### Cut layer
#以后可以延伸的知识/切割层
在这种情况下，每个客户端通过一个深度神经网络执行前向传递，一直计算到一个特定的层，这个层被称为“切割层”（cut layer）。这个切割层之前的部分被用来提取特征或进行一些中间计算，而不涉及最终的输出预测。这种方法通常用于联邦学习等场景，其中不同的客户端计算局部更新，然后这些局部更新被集成到全局模型中。在这个过程中，切割层的选择和使用可以根据具体的任务和需求进行调整。

### Back propagated
#以后可以延伸的知识/反向传播
反向传播（Backpropagation）是神经网络训练过程中的一种优化算法。它用于调整神经网络中每个神经元的权重，以最小化预测输出与实际输出之间的误差。这一算法的核心思想是从神经网络的输出层向后传播误差信号，然后根据这些误差信号来更新权重，使网络的预测逐渐接近实际目标。

反向传播的步骤如下：
1. 正向传播：首先，通过输入数据将信号从网络的输入层传播到输出层，计算出网络的预测输出。

2. 计算误差：然后，将网络的预测输出与实际目标进行比较，计算出误差，通常使用损失函数来度量误差的大小。

3. 反向传播误差：接下来，从输出层开始，将误差信号向后传播到网络的隐藏层和输入层。这是通过链式法则计算每一层的误差信号，以确定每个神经元对误差的贡献。

4. 更新权重：最后，使用误差信号来更新每个神经元的权重，以减小误差。通常采用梯度下降等优化算法来进行权重的调整，以使网络的预测更接近实际目标。

这个反复迭代的过程持续进行，直到网络的预测误差达到满意的程度或收敛到某个阈值。通过反向传播，神经网络可以学习如何逐渐提高其性能，以执行各种任务，如图像分类、语音识别和自然语言处理。

### Vanilla Spilt Learning
#以后可以延伸的知识/分割学习
![[Pasted image 20231011140151.png]]
Vanilla Split Learning（简称Split Learning）是一种隐私保护的机器学习方法，特别适用于联邦学习和分布式学习场景。它的主要目标是在多个数据持有方之间共享模型训练的信息，同时保持用户数据的隐私。

在Split Learning中，模型被分成两个或多个部分，通常是神经网络的层次结构。这些部分分别由不同的数据持有方（例如用户设备、机构或服务器）拥有，而不共享原始数据。训练过程分为两个主要阶段：

1. **前向传播阶段**：在这个阶段，每个数据持有方使用其拥有的模型部分来执行前向传播，将输入数据通过模型传递，生成部分的中间结果。然后，这些中间结果被传递给下一个数据持有方。

2. **反向传播阶段**：在这个阶段，各个数据持有方接收到上一方传递的中间结果，并执行反向传播，计算梯度信息。然后，这些梯度信息被传递回前一个数据持有方，用于更新模型参数。这个过程在各个数据持有方之间循环执行，直到模型收敛为止。

Split Learning的关键优点是，原始数据不离开各个数据持有方的设备或服务器，因此用户的隐私得到了很好的保护。只有模型的中间信息在数据持有方之间传递，这些信息通常不足以泄露敏感数据。这使得Split Learning成为隐私保护要求较高的场景中的一种有吸引力的机器学习方法。

总之，Vanilla Split Learning是一种用于隐私保护的分布式机器学习方法，它通过将模型分成多个部分，并在不共享原始数据的情况下协同训练这些部分，从而保护用户数据的隐私。

### U-shaped split learning
#以后可以延伸的知识/U型分割学习
![[Pasted image 20231011140306.png]]
U-shaped Split Learning是Split Learning（分割学习）的一个变体，用于隐私保护的分布式机器学习。与传统的Split Learning不同，U-shaped Split Learning引入了一个额外的环节，以增强模型训练的效率和隐私保护。

在U-shaped Split Learning中，模型被分成两个部分，分别由两个不同的数据持有方（例如用户设备或数据中心）拥有，这两个部分通常构成了一个"U"形的结构。训练过程包括以下阶段：

1. **前向传播阶段**：每个数据持有方执行前向传播，将输入数据通过自己拥有的模型部分传递。在U-shaped Split Learning中，这个过程通常包括了"跳跃连接"，其中一方的输出作为另一方的输入，形成了U形的连接。

2. **反向传播阶段**：反向传播是在两个数据持有方之间进行的。首先，一个方执行反向传播来计算梯度信息，并将部分的梯度传递给另一个方。然后，另一个方继续执行反向传播，结合接收到的梯度信息，计算出完整的梯度并更新模型参数。

U-shaped Split Learning的关键特点是在模型的中间层引入了跳跃连接，这允许两个数据持有方之间进行信息交换，从而更加高效地进行模型训练。这种结构可以降低训练的通信开销，同时仍然保持用户数据的隐私，因为原始数据不会离开各自的数据持有方。

总的来说，U-shaped Split Learning是一种用于分布式机器学习和隐私保护的方法，它在传统的Split Learning基础上引入了U形的连接结构，以提高训练效率和隐私保护水平。

### Data decorrelation
#以后可以延伸的知识/数据相关性
在机器学习和数据处理中，数据的decorrelation指的是通过某种方式减少数据之间的相关性或相似性，使得数据更加独立。当数据经过decorrelation处理后，它们的相关性较低，可以更容易地被分割或分开，这对于某些任务和模型训练可能是有益的。

在数据中引入decorrelation，从而使得数据在一个较早的阶段就可以进行分割。这可以在某些情况下提高机器学习模型的训练效率或任务执行效果。

### 激活通道
#以后可以延伸的知识/激活通道
在机器学习和神经网络中，激活通道是神经网络中的一种组件，用于处理输入数据并传递信息。修剪通道意味着删除或减少网络中的某些激活通道，从而降低了模型的复杂性和参数数量。这可以减少在分割学习期间需要传输的信息量，提高通信效率。

这种方法是工程驱动的，意味着它是为了解决实际工程问题而提出的，可能是为了在分割学习中降低通信成本或提高性能而采取的策略。

### Non-IID data
#以后可以延伸的知识/非独立同分布数据
在提到分布式学习中的非独立同分布（non-IID）数据时，通常指的是不同客户端i和j之间的Pi和Pj之间存在差异。然而，也值得注意的是，分布Q和Pi可能随着时间的推移而发生变化，引入了另一个“非独立同分布”的维度。

简单来说，非独立同分布数据是指在不同客户端之间或随着时间推移，数据的分布存在差异。这可以是因为不同客户端拥有不同类型或不同分布的数据，或者因为数据的分布随着时间的推移而发生变化。在分布式学习中，处理非独立同分布数据是一个重要的挑战，因为模型需要适应这些数据的变化和差异，以实现良好的性能。

### Pathological but degenerate non-IID distributions
#以后可以延伸的知识/异常但退化的非独立同分布数据分布
在分布式学习或联邦学习中，"pathological but degenerate non-IID distributions" 指的是一种异常但退化的非独立同分布数据分布。让我们逐步解释这些术语的含义：

1. "Pathological"：这表示数据分布的某些方面异常或不正常，可能与特定情况、数据源或数据采集方式有关。这可能意味着某些客户端的数据非常不同于其他客户端，或者存在某种不寻常的数据分布模式。

2. "Degenerate"：在数学和统计学中，"degenerate" 表示某个问题或情况陷入了一个非常特殊或简单的状态。在这里，它指的是非独立同分布的情况非常特殊或异常。

3. "Non-IID"：这表示数据在不同客户端之间的分布不同。不同客户端的数据可能有不同的特征、分布、标签等，不像传统的统计假设中的独立同分布（IID）数据。

"Pathological but degenerate non-IID distributions" 的意思是，数据分布在某种情况下异常或不正常，但这种情况非常特殊和简单，可能导致数据分布之间的非独立同分布情况变得微不足道或易于处理。这个概念通常用于描述某些不寻常但在实践中可能出现的数据分布情况，研究人员可能需要针对这些情况采取特殊的处理方法。

### Primitives
#以后可以延伸的知识/原语
在计算机科学和计算领域，"原语" 是指一种基本操作或原子操作，它不能再分解成更小的操作。原语通常是计算机系统中的基本构建块，用于执行特定任务或操作。原语是具有原子性的，这意味着在执行原语时，不会被中断或分割成更小的步骤。

原语通常用于并发编程或多线程环境中，以确保多个线程之间的互斥和同步。例如，锁定和解锁操作可以是一种原语，用于确保只有一个线程能够访问某个共享资源。原语的使用有助于避免竞态条件和其他并发问题。

"聚合原语" 是指一种用于汇总和处理多个数据元素或结果的基本操作。在分布式计算和联邦学习中，聚合原语用于将来自不同参与方或节点的数据或模型参数进行合并和汇总。

在联邦学习中，聚合原语通常用于将本地模型参数的更新合并成全局模型。一种常见的聚合原语是加权平均，其中每个本地更新根据其权重与其他更新相结合以更新全局模型。其他聚合原语包括最大值、最小值、求和等，具体取决于应用场景和算法的需求。

聚合原语在分布式系统中起到重要作用，可用于处理大规模数据、联邦学习、分布式机器学习等应用中。它们有助于将多个分布式节点的信息整合在一起，以便有效地合作和协同处理数据或模型。

### H-smooth
#以后可以延伸的知识/H-smooth
H-smooth 函数： H-smooth 函数是一种数学函数，它在局部区域内具有有界的 Lipschitz 连续性梯度。这意味着函数的梯度变化不会太剧烈，可以更容易地进行数值优化。

更正式地说，通过“H-smooth”，我们指的是对于所有的z，函数f(·; z)都是可微的，并且具有H-Lipschitz梯度，也就是对于所有的x、y选择：

f(x; z) - f(y; z) ≤ ∇f(y; z) · (x - y) + (H/2) * ||x - y||^2

这个不等式表示了 H-smooth 函数的性质。它表明，函数f在局部区域内的变化受到了梯度的 Lipschitz 连续性限制，其中H是 Lipschitz 常数。这意味着函数f的梯度变化不会太快，使得在数值优化中更容易处理。

这个性质在优化中很重要，因为它与函数的光滑性和收敛性有关。如果一个函数是H-smooth，那么在使用数值优化算法时，通常更容易找到全局最小值或局部最小值。

ps：Lipschitz连续性是指在数学和数值分析中，一个函数的性质，表示其梯度或导数在给定区域内的变化受到一定限制。具体来说，一个函数被称为Lipschitz连续，如果存在一个常数L（称为Lipschitz常数），使得在该函数的定义域内，函数值之间的变化满足以下不等式：

|f(x) - f(y)| ≤ L * |x - y|

这表示在给定区域内，函数值之间的变化不会太剧烈，其斜率或变化率受到Lipschitz常数L的限制。这个性质在数值优化、微分方程求解和机器学习等领域中经常用于分析算法的收敛性和稳定性。

### Bounded gradient variance
#以后可以延伸的知识/有界梯度方差
"有界梯度方差"的假设是在随机梯度方法的上下文中使用的，它意味着在训练过程中，损失函数梯度的方差（或波动）受到限制或控制。这一假设在优化算法的分析中被广泛使用，特别是在客户端（或工作者）执行随机局部更新的情况下。

以联邦学习为例，每个客户端会处理其本地数据并根据其本地模型计算梯度。有界梯度方差的假设意味着由于本地更新，不同客户端计算的梯度不会出现过多的变化或极端波动。当这一假设成立时，可以更容易地分析联邦学习算法如何收敛到全局模型。

通过对梯度方差进行限制，可以更容易地提供关于优化算法的收敛性质的理论保证，包括联邦学习中使用的算法。这一假设确保了梯度没有过多的噪音，这对于学习过程的稳定性和可预测性很重要。

### Momentum
#以后可以延伸的知识/动量
动量可以加速收敛速度的原因在于它允许梯度更新在不同迭代中具有一定的持续性。具体来说，动量方法在每次迭代中不仅利用当前梯度信息，还考虑了之前迭代的方向和速度。这有助于跨越局部极小值，加速收敛并减少震荡。动量的作用类似于给优化问题中的搜索过程增加了一定的"惯性"，从而更好地穿越峡谷和平原地带，最终更快地收敛到全局最小值。

在联邦学习中，各客户端维护局部动量缓冲区和局部模型参数，这有助于平滑梯度更新，特别是在客户端之间的通信周期之间。然而，证明动量在联邦学习中确实加速了收敛速度可能会更具挑战性，因为联邦学习涉及到多个分布式客户端和通信延迟，其动态复杂性较高。所以，研究人员需要开展详尽的数学分析和实验研究以验证在特定情况下动量是否加速了联邦学习的收敛速度。通过在联邦学习中引入本地动量，可以更好地处理不同客户端的局部梯度信息，从而提高全局模型的训练效果。这种方法有助于克服通信效率低下的问题，同时保持模型的收敛性。

### Adagrad
#以后可以延伸的知识/Adagrad
Adagrad是一种自适应学习率的梯度下降方法，它允许根据每个参数的历史梯度信息来调整学习率。在这个方案中，每个客户端使用Adagrad来执行本地梯度更新，然后在通信轮次中将它们的本地参数合并以更新全局模型。这种方法有助于提高训练的性能和收敛速度，尤其是当客户端的局部数据和梯度分布不均匀时。

### Spectrum
#以后可以延伸的知识/关系谱
"关系谱"（spectrum）通常指的是一系列有序或连续的事物的不同状态或程度。在上下文中，关系谱指的是在模型共享和个性化之间的不同程度或点。在此处，它表示可以根据需要在全局模型和每个客户端都有不同模型之间进行连续的调整。这个关系谱用于说明在联邦学习中可以采取不同的模型设计策略，根据任务和客户端的需求来调整模型的共享程度。

### Interpolations
#以后可以延伸的知识/插值
插值是一种数学和计算方法，用于在已知数据点之间估计未知点的值。它可以在数学、统计学和计算机科学中应用广泛。插值通常用于构建连续函数，以便在给定数据点之外的位置估计函数值。这有助于填充数据的缺失部分或生成平滑的曲线。

一种常见的插值方法是线性插值，它通过连接已知数据点之间的直线来估计未知点的值。还有其他高级插值技术，如多项式插值、样条插值和三次样条插值，这些方法使用更复杂的函数来逼近数据点之间的曲线。插值的目标是使插值曲线在已知数据点处尽可能接近原始数据。

在联邦学习中，插值可以用于平滑和调整全局模型以适应不同客户端的数据，从而实现更好的模型性能和收敛性能。

### learning-to-learn (LTL)
#以后可以延伸的知识/学习如何学习
标准的"学习如何学习"（Learning-to-Learn，LTL）设置是指在任务之间具有一个元分布（meta-distribution），从中采样用于学习一个学习算法的任务。这意味着我们不仅要学习一个模型来解决特定的任务，还要学习如何设计一个学习算法，以便在未来遇到新任务时能够更有效地学习和适应。

在LTL中，通常有两个层次的学习过程。第一层是任务学习，这是指学习如何在给定的任务上执行学习，训练模型以满足特定任务的要求。第二层是元学习，这是指学习如何在不同的任务之间进行任务学习，以便在遇到新任务时能够更快速地适应和学习。

这个方法的目标是提高模型的泛化能力，使其能够在新任务上表现良好，而不仅仅是在已知的任务上。元学习是一种强化学习和迁移学习的方法，用于培养模型具有更好的适应性和通用性。

### Meta-distribution
#以后可以延伸的知识/元分布
元分布（meta-distribution）是一个概率分布，用于描述不同任务或数据生成过程的分布情况。在机器学习和元学习领域，元分布通常用来表示任务之间的变化或任务分布，而不是特定任务的数据点。

元分布可以用来描述在不同任务或领域之间的数据分布差异，这对于元学习等问题很重要，因为它们旨在让模型能够适应不同的任务或领域。通过了解元分布，可以更好地为模型设计适应性和泛化性，以在新任务上表现良好。

在元学习中，通常假设存在一个元分布，从中采样得到不同任务的数据集。模型在这些不同任务上进行训练，以提高其适应性和泛化能力。因此，元分布用于描述任务之间的变化和数据生成过程的分布特征。

### Reptile algorithm
#以后可以延伸的知识/Reptile
"Reptile"是一种机器学习算法，用于元学习（meta-learning）问题。元学习是一种机器学习范式，其中模型需要通过学习一系列不同任务，以提高其在新任务上的泛化性能。

在上述引用中，提到Reptile算法的训练阶段与Federated Averaging（联邦平均）有关。Federated Averaging是一种用于分布式机器学习的算法，允许多个客户端协同训练一个全局模型，而每个客户端可能有不同的本地数据。

Reptile算法与Federated Averaging类似，因为它也涉及到训练一个全局模型，并且在训练中考虑了不同任务。然而，两者之间的主要区别在于Reptile算法通常假设所有客户端（或任务）具有相同数量的数据，而Federated Averaging可以应对客户端拥有不同规模和类型的数据的情况。

综上所述，Reptile算法和Federated Averaging都是与多任务学习和元学习相关的算法，但它们在处理数据分布和任务之间的不同方面存在差异。

### Concept drift
#以后可以延伸的知识/概念漂移
在联邦学习的背景下分析概念漂移等问题是可能的，同时也可以将其与终身学习相联系。概念漂移指的是数据的统计特性随时间发生变化的现象，需要机器学习模型适应这些变化。

在联邦学习中，特别是在从各种设备或客户端收集数据的情境下，概念漂移可能是一个重要挑战。每个客户端可能会以不同的速度或不同的方向经历概念漂移。客户端之间概念漂移的异质性可能会影响联邦模型的整体性能。

终身学习是一种处理连续流数据的机器学习范例，旨在使模型适应数据分布的变化或新概念的出现。联邦学习，尤其是当应用于不断变化的数据源时，可以被看作是终身学习的一种形式。

在联邦学习中分析概念漂移可能包括研究客户端数据分布随时间如何变化，以及联邦模型如何适应这些变化。还可能涉及探索联邦终身学习的技术，其中模型不断学习和适应不断变化的数据。

总之，虽然概念漂移和联邦学习之间存在内在联系，但在联邦学习中分析和解决概念漂移可以借鉴终身学习原则，以创建更具适应性和稳健性的联邦模型。

### Record linkage
#以后可以延伸的知识/记录链接
记录链接（record linkage）是一种将来自不同数据源的记录或数据元素关联到同一实体或个体的过程。这在数据管理和数据分析中非常重要，因为它允许将来自不同来源的信息合并在一起，以便进行分析、建模或其他数据处理任务。通常在处理联邦学习等任务时，需要解决来自不同参与方的数据的记录链接问题，以便有效地进行协同学习。

### Secure Aggregation
#以后可以延伸的知识/安全聚合
安全聚合（Secure Aggregation）是一种隐私保护技术，用于在多方之间对数据进行聚合，而不泄露个别数据的详细信息。在这个背景下，"multiset-singletons" 指的是多方各自的数据集，而 "multiset-sum" 表示将这些数据集相加，以进行聚合操作。在典型情况下，为了保护隐私，每个数据项会被包装成 multiset-singleton，然后进行安全的 multiset-sum 操作以获得结果，而不会泄露数据的详细信息。

安全的 multiset-sum 操作确保了每方都可以将其数据贡献到聚合中，但不会知道其他方具体贡献了什么数据。这是在联邦学习等多方合作的环境中保护隐私的一种方式。

对于安全 shuffling，它通常涉及对多方的数据进行重新排列，以在不泄露数据内容的情况下创建混淆数据集。虽然安全 shuffling 和安全聚合都是隐私保护的关键技术，但它们在实现方式上可能有所不同，因为它们的操作和目标略有不同，但在典型操作情境下，通常都需要高效的实现。

### The Onion Router
#以后可以延伸的知识/洋葱网络
Tor（The Onion Router）网络是一种用于匿名浏览和通信的开放网络。它旨在保护用户的隐私，通过将其网络流量路由通过一系列随机选择的中间节点（也称为中继或节点）来实现匿名性。这些中间节点是由志愿者维护的，它们一起构成了一个分布式的网络，其中每个节点仅知道相邻的节点。

当您连接到Tor网络时，您的数据通过一层一层的中继传递，类似于剥洋葱的层叠。这使得难以追踪数据流的原始来源，从而增加了用户的隐私。同时，Tor还可以让用户访问被封锁或审查的网站，因为它可以绕过特定地区的互联网过滤。

需要注意的是，虽然Tor网络可以提供一定程度的匿名性，但它并不是绝对的，也有一些局限性。此外，Tor网络也可以被滥用，因此它是一个受到法律和伦理规定的领域。用户在使用Tor时需要理解其用途，以确保合法合规的使用。

### Zero knowledge
#以后可以延伸的知识/零知识证明
零知识证明是一种加密学的概念，用于建立隐私和安全性。在零知识证明中，有两个主要角色：证明者和验证者。

1. 证明者（Prover）：这是一个拥有某些秘密信息的方。证明者希望向验证者证明某个声明是真实的，但不希望揭示任何与该声明相关的具体信息。

2. 验证者（Verifier）：这是希望确认证明者的声明是否有效的方。验证者不知道证明者的秘密信息，但希望通过零知识证明来确认声明的真实性。

零知识证明的核心思想是证明者可以以一种特殊的方式生成证明，这个证明可以使验证者相信某个陈述是真实的，但不会泄露任何有关如何实现该陈述的具体信息。实际上，证明者可以向验证者提供一个证据，证明他们知道如何执行某个任务（例如解密一个加密文件）而不必实际执行该任务或透露解密密钥。

零知识证明在许多领域都有应用，包括密码学、区块链、数据隐私和身份验证。它们允许在验证信息的同时保护个人的隐私和敏感信息，因为验证者无需了解实际的敏感信息，只需知道陈述是否为真即可。这为建立信任和确保安全性提供了一种有力的方式。

###  Softmax
#以后可以延伸的知识/softmax
[Site Unreachable](https://machinelearningmastery.com/softmax-activation-function-with-python/)

Softmax输出层是深度学习神经网络中的一种常见输出层类型，通常用于多类别分类任务。它的作用是将网络的原始输出转化为概率分布，使得每个类别的预测概率之和为1。

Softmax输出层通常用于最后一个全连接层或卷积层之后。在该层中，神经网络的原始输出（也称为logits）经过softmax函数的转换，将其映射为一个包含各个类别概率的向量。

Softmax函数的数学表达式如下：

对于给定的输入向量 z = [z1, z2, ..., zn]，Softmax 函数的输出 y = [y1, y2, ..., yn] 计算如下：

y_i = exp(z_i) / Σ(exp(z_j))，其中 i = 1, 2, ..., n

其中，exp(z_i) 表示 e 的指数幂，Σ(exp(z_j)) 表示所有类别的指数幂之和。这个操作将原始输出转化为在0到1之间的概率值，并确保所有类别的概率之和为1。

Softmax输出层的预测结果通常是具有最高概率的类别，即预测为具有最大概率的类别。这使得神经网络适用于多类别分类任务，如图像分类、文本分类等。 Softmax输出层的输出可以用于计算损失函数，并进行反向传播以进行模型训练。

### Computation offloading
#以后可以延伸的知识/计算卸载
计算卸载（computation offloading）是一种计算模型，它涉及将计算任务从一个设备（通常是移动设备或边缘设备）传送到另一个更强大的计算资源，如云服务器或边缘服务器，以在那里执行。这通常发生在移动计算环境中，其中移动设备的计算能力有限，而远程服务器或云计算资源具有更大的计算能力。

计算卸载可以用于优化性能、降低能耗和提高用户体验。例如，当一个移动应用程序需要进行复杂的计算时，可以将这些计算任务卸载到云服务器上执行，然后将结果传输回移动设备。这有助于减轻移动设备的负担，延长电池寿命，同时仍然能够提供高性能的计算。

计算卸载还可以在边缘计算场景中使用，其中计算任务可以分布在边缘服务器和云服务器之间，以减少延迟并更好地处理实时数据。这种方法有助于提高系统的响应速度和效率。

### Cell association
#以后可以延伸的知识/小区关联
Cell association（小区关联）是无线通信领域的一个概念，指的是移动设备（如手机）决定连接到哪个无线基站或小区的过程。在无线通信网络中，小区通常由一个基站提供覆盖，基站通常会划分成多个小区以有效地覆盖大面积。

当一个移动设备在一个无线通信网络中移动时，它需要决定与哪个小区建立连接，以便进行通信。这个决策通常基于一些因素，包括信号强度、信噪比、小区拥塞情况等。设备会选择与信号最强或最适合的小区建立连接，以确保良好的通信质量和数据传输速度。

Cell association对于移动通信非常重要，因为它直接影响到设备的通信性能和网络的负载均衡。通过智能地选择合适的小区，可以提高通信效率并减少网络拥塞问题。这是移动通信系统中的基本概念，用于优化网络资源的利用。

### Stackelberg game
#以后可以延伸的知识/Stackelberg 
模型拥有者是买家，而参与者是卖家。在下层子博弈中，模型拥有者确定训练数据的规模，以最大化利润，考虑到学习模型的准确性与训练数据规模之间的递增凹函数关系。

同时，为了确保博弈合理，每份合同必须满足个体合理性 (Individual Rationality, IR) 和激励兼容性 (Incentive Compatibility, IC) 的约束条件。个体合理性确保每个参与者不会因合同而损失，而激励兼容性确保参与者愿意按照合同行事，而不会试图通过不合理的方式获取更大的回报。这些约束条件有助于确保博弈的公平性和有效性。

### Generative Adversarial Networks
#以后可以延伸的知识/生成对抗网络

![[Pasted image 20231030205401.png]]

GAN包括两个主要组件：生成器网络和判别器网络。生成器网络尝试通过向真实数据添加一些“噪音”来生成虚假数据。然后，生成的虚假数据传递给判别器网络进行分类。在训练过程结束后，GAN可以生成具有与训练数据集相同统计属性的新数据。这使得GAN能够生成看起来类似于真实数据的虚假数据。这个过程有助于生成逼真的虚假数据，可以用于各种应用，如图像生成、自然语言处理等。

### Double Deep Q-Network
#以后可以延伸的知识/DDQn 
DDQN代表"Double Deep Q-Network"，它是强化学习中一种用于训练智能代理（通常是机器学习代理）的深度学习算法。DDQN是基于深度Q-网络（DQN）的改进版本。

DQN是一种强化学习算法，用于训练智能代理来学习最佳动作策略以最大化长期奖励。它通过估计一个动作值函数（Q函数）来完成这一任务。DQN使用神经网络来逼近Q函数，从而能够处理高维状态空间的问题。

DDQN的改进之处在于，它引入了两个Q网络（一个用于选择最佳动作，另一个用于评估动作价值）。这两个网络交替地进行学习，一方面选择最佳动作，另一方面评估这个动作的价值。这种双网络结构有助于减少估计Q值时的偏差，提高了算法的稳定性和性能。

总之，DDQN是一种强化学习算法，用于改进Q-learning，并且在处理高维状态空间的问题以及提高学习效率方面表现出色。

### Q-learning algorithm
#以后可以延伸的知识/Q学习 
Q-learning算法是一种基于强化学习的机器学习算法，用于解决马尔可夫决策过程（MDP）中的问题。它是强化学习中的一种经典方法，旨在帮助智能体（agent）学习如何在不确定环境中做出最佳的决策，以最大化长期奖励。

Q-learning算法的核心思想是学习一个价值函数，通常称为Q值函数（Q-function）。该函数对每个状态-动作对分配一个值，表示在特定状态下采取某个动作后所能获得的累积奖励的期望值。Q-learning通过迭代地更新Q值函数，以逐渐改进智能体的策略。

Q-learning的主要算法步骤如下：

1. **初始化Q值函数：** 初始化Q值函数，可以为每个状态-动作对分配一个初始估计值，通常为0。

2. **选择动作：** 在当前状态下，根据Q值函数选择一个动作。通常使用ε-greedy策略，以便在探索（exploration）和利用（exploitation）之间取得平衡。ε是一个小的正数，表示以ε的概率进行探索，以1-ε的概率进行利用。

3. **执行动作并观察奖励：** 执行选定的动作，进入下一个状态，并观察从环境中获得的奖励。

4. **更新Q值函数：** 使用以下公式来更新Q值函数：
   
   Q(s, a) = Q(s, a) + α * [R + γ * max(Q(s', a')) - Q(s, a)]
   
   其中，Q(s, a)表示当前状态s和动作a的Q值，α是学习率（learning rate），R是观察到的奖励，γ是折扣因子，s'是下一个状态，a'是在下一个状态下选择的最佳动作。

5. **迭代：** 重复步骤2至步骤4，直到Q值函数收敛或达到一定的迭代次数。

Q-learning的目标是学习一个最优的Q值函数，使得在任何状态下，选择Q值最高的动作会导致最大的累积奖励。一旦Q值函数学习完毕，智能体可以根据它来选择最佳动作，从而优化它的策略。

Q-learning算法广泛应用于很多领域，包括机器人控制、自动驾驶车辆、游戏玩法优化等需要序列决策的问题。

### Echo State Network
#以后可以延伸的知识/回声状态网络
ESN代表"Echo State Network"，它是一种递归神经网络（RNN）的变种，用于处理时间序列数据和机器学习任务。ESN算法以其简单性和强大的序列建模能力而闻名。

与传统的RNN不同，ESN有三个主要组成部分：

1. 输入层：接收输入信号。

2. 隐藏层（固定权值）：包含一个大型的随机生成的神经元池，这些神经元的连接权值通常是固定的，不进行训练。这是ESN的关键特点，因为它允许网络在保持大部分权值不变的情况下，从输入数据中提取特征。

3. 输出层：可以是全连接层，它将隐藏层的输出映射到所需的输出。

ESN的训练相对较简单，通常只需要训练输出层的权值，而隐藏层的权值是随机生成的并在整个训练过程中不进行更新。这使得ESN在许多时间序列预测、分类和序列建模任务中表现出色。

ESN的关键思想是，通过隐藏层中的随机神经元池，网络能够捕获输入数据的动态特征，而输出层的训练则使网络适应具体任务。这种分离隐藏层和输出层训练的方法使ESN成为一种强大的工具，特别适用于时间序列数据的处理。

### Mean-field game
#以后可以延伸的知识/均场博弈
Mean-field game（均场博弈）是一种博弈论和动态系统理论相结合的数学框架，用于描述大规模智能体（agents）的互动，这些智能体受到群体行为的影响。它在经济学、社会科学、交通规划、能源管理等领域中具有广泛的应用。

在均场博弈中，每个智能体试图优化自己的策略，同时考虑到整体群体行为的影响。智能体的策略是根据某些目标函数和约束条件来确定的。这些目标函数通常包括个体收益或效用，以及与其他智能体的互动相关的项。

均场博弈的关键特征是，它考虑了大量相似的智能体，而不是仅仅处理有限数量的对手。均场博弈的一个典型假设是智能体的策略和行为受到群体的平均行为的影响，这种平均行为通常以均场概率分布的形式建模。

均场博弈的目标是找到群体行为的均衡，即一组策略，使得每个智能体的策略是最佳响应均场分布的。通过研究均场博弈，我们可以理解大规模群体中的协同行为、竞争和资源分配问题，这对于许多实际应用具有重要意义。

均场博弈的数学理论和方法通常涉及到偏微分方程、动力系统、最优控制和概率论等领域，以描述群体行为和策略的演化。均场博弈在多个领域中有着广泛的研究和应用，以解决复杂的社会和经济问题。

### Markov decision process
#以后可以延伸的知识/马尔可夫决策过程
Markov Decision Process（MDP，马尔可夫决策过程）是一种数学框架，用于建模具有随机性和决策性质的序列决策问题。MDP通常用于强化学习（Reinforcement Learning）和运筹学等领域，以解决涉及不确定性的问题，如机器智能、控制系统、资源分配等。

MDP的关键组成部分包括以下元素：

1. **状态空间（State Space）：** 描述问题中可能的状态的集合。在MDP中，状态通常代表系统或环境的某种特定配置。

2. **动作空间（Action Space）：** 描述在每个状态下可用的决策或动作的集合。智能体或决策制定者需要在每个状态下选择一个动作来影响系统的演进。

3. **状态转移概率（State Transition Probabilities）：** 描述在某个状态下采取某个动作后系统进入下一个状态的概率分布。这反映了环境的不确定性和随机性。

4. **奖励函数（Reward Function）：** 为每个状态-动作组合分配一个奖励值，表示在采取特定动作后从某状态转移到另一状态时的即时奖励。奖励函数用于评估动作的质量。

5. **折扣因子（Discount Factor）：** 用于表示智能体对未来奖励的重要性。它决定了在长期决策中，智能体更关注即时奖励还是未来奖励。

MDP的目标是找到一种策略（Policy），即从每个状态选择动作的策略，以最大化期望累积奖励（或价值函数）。通常，这个目标可以用值函数（Value Function）来表示，它衡量了从每个状态开始采取某个策略后所能获得的累积奖励的期望值。

MDP是许多强化学习算法的核心，包括Q-learning、Deep Q-Networks（DQN）等。它在机器学习、人工智能和控制系统等领域中有广泛的应用，用于解决需要进行序列决策的问题，例如智能机器人导航、自动驾驶车辆控制、资源管理等。

