## **1. 联邦学习的背景**

联邦学习是近年来兴起的一种**分布式机器学习技术**，其提出背景是现实生活中**数据难以集中管理**、**隐私安全问题**突出以及机器学习算法本身的局限性。[What is Federated Learning, 联邦学习(一、联邦学习概述) - 知乎](https://zhuanlan.zhihu.com/p/611394736)

## **2. 联邦学习的概念**

联邦学习：**联邦学习**是一种**分布式机器学习模型**，本质上是通过多个用户设备**共同训练**一个代表**所有用户设备**的**全局模型**，而训练的过程**不需要用户数据的交换**，较常见的分布式机器学习而言更**强调隐私性**。

### **2.1  Characteristics of FL**
Universality for cross-organizational scenarios

Massively Non-Identically Independent Distribution (Non-IID)

Decentralized technology

Equality of status for each node

## **3. 联邦学习的分类**

不同类型的联邦学习方法主要根据**数据分布**和**数据拥有者之间的差异**而划分。主要分为**横向**联邦学习、**纵向**联邦学习和**迁移**联邦学习。

### **3.1 横向联邦学习**

**横向联邦学习的概念**

#横向联邦学习
**横向联邦学习**：每个参与者之间的数据源之间的**特征相同**，但数据**分布不同**。例如，多家医院共同合作来训练一个用于癌症诊断的模型，所有的患者都符合""癌症"这个病症的数据特征，但是每家医院可能拥有不同的患者数据。

>  In the case of horizontal FL, there is a certain amount of overlap between the feature of data spread across various nodes, while the data are quite different in sample space.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=10&selection=31,1,33,26|CAIE_A review of applications in federated learning_deposit, page 10]]

**横向联邦学习的训练过程：**

横向联邦学习可以看作是一种**数据并行**的分布式机器学习框架。即每个用户设备上都有一个用户自己的数据集，用户设备上持有的**用户数据是不同的**，而每个用户设备都有训练**一个相同的模型**。这个模型与服务器上的全局模型是一致的。

每个客户端在**本地**用自己的数据去**训练**这个模型，得到的模型参数通过网络信道**发送给服务器**。服务器对所有（或部分）用户设备的模型进行**聚合**，得到精度较高的、符合多个用户利益的**全局模型**。

在这个过程中，用户之间没有数据交换，保障了用户之间的**隐私性**。且每个用户都可以作为一个独立的个体，在本地运行经过协同训练后的模型，提高了**模型的性能**。

**训练步骤**如下：

1. 各个用户设备从服务器**下载**最新的模型。
2. 每个用户设备用自己的数据在本地进行模型**训练**。
3. 各个用户设备将本地迭代好的模型**上传**给服务器。
4. 服务器收集发送上来的所有模型，进行**模型聚合**。
5. 继续重复上述步骤，直至训练停止。

**横向联邦学习挑战及前人的对策**
#### **1.limited labeled entities**
>  Gao et al., (2019) introduced hierarchical heterogeneous horizontal FL frame. 

[[CAIE_A review of applications in federated learning_deposit.pdf#page=11&selection=6,17,7,10|CAIE_A review of applications in federated learning_deposit, page 11]]



### **3.2 纵向联邦学习**

**纵向联邦学习的概念**

#纵向联邦学习
**纵向联邦学习**：每个参与者之间的数据源之间的**特征不同**，但数据**分布相同**。例如，不同保险公司之间合作训练一个用于预测疾病风险的模型，其中每家保险公司可能只有一部分特征数据，但这些公司的样本数据都来自同一个数据源。

>  All the parties hold homogeneous data which means they have partial overlap on sample ID whereas differ in feature space.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=11&selection=20,50,22,6|CAIE_A review of applications in federated learning_deposit, page 11]]

**纵向联邦学习的训练过程**

纵向联邦学习本质上是多个参与方**共同完成一件事情**，把这件事情拆成多个部分去逐一分配给每个参与方。可以看作是一种**模型并行**的分布式机器学习框架。即每个用户设备上都有模型的一个部分，训练时需要用**同一批量数据**去让每个用户逐一得出自己那一部分的结果，然后按顺序将最终结果推导出来。

**训练步骤**如下：

1. 所有用户设备进行**数据对齐**，选择**同一个批量**的数据。
2. 将数据按**特征**分成不同的切片，每部分切片**下放**给对应的用户设备。
3. 用户设备拿到自己那部分的数据特征，用本地的模型分片进行**训练**。
4. 全局模型用每一个模块**聚合**完整的模型
5. 全局模型将每个模块的更新方式**下放**给每个参与者，参与者们完成本地更新
6. 重复以上步骤，直至训练结束。

**纵向联邦学习挑战及前人的对策**
#### **1.the correspondence between different owners**
> a modified token-based entity resolution algorithm to preprocess vertical partitioned data, powered by Nock et al., (2018).

[[CAIE_A review of applications in federated learning_deposit.pdf#page=12&selection=18,43,19,82|CAIE_A review of applications in federated learning_deposit, page 12]]

> Hardy et al., (2017) designed an end-to-end scheme on linear classifier and applied additive homomorphic encryption to defense honest-but-curious adversary for vertical FL. 

[[CAIE_A review of applications in federated learning_deposit.pdf#page=12&selection=20,0,22,4|CAIE_A review of applications in federated learning_deposit, page 12]]

### **3.3 迁移联邦学习**

**迁移联邦学习的概念**

#迁移联邦学习
**迁移联邦学习**：当每个参与者之间的数据源之间的**特征部分重叠**，**数据分布**也部分**重叠**时，以提高模型的泛化能力。例如，将在一个数据分布上训练的模型应用于另一个数据分布上，以提高模型在新数据上的表现。

> data shares neither sample space nor feature space

[[CAIE_A review of applications in federated learning_deposit.pdf#page=13&selection=2,12,2,62|CAIE_A review of applications in federated learning_deposit, page 13]]

**迁移学习简介**

迁移学习是指将从一个任务中学习到的知识或经验应用到另一个任务中的机器学习技术。

迁移学习的本质是通过利用源领域和目标领域之间的相关性，将源领域中学到的知识和经验应用于目标领域中，从而提高目标任务的性能。通常情况下，源领域和目标领域存在一定的相似性，例如数据分布、特征空间、任务目标等方面的相似性。

**联邦迁移学习的步骤**

迁移联邦学习本质上是学习**不同参与方**上的样本特征的**共同之处**。**迁移联邦学习**与**纵向联邦学习**的**结构**完全相同，不同点是**传递的数据不同**。纵向联邦学习只需要每个参与方关注自己本地的训练及更新结果，而由于迁移联邦学习需要多个参与方协作学习数据中的共同特征，因此数据传输时需要参与方彼此先沟通好，得到数据中的共同特征后。再通过服务器辨别共同特征和非共同特征后进行全局模型的学习。

**迁移联邦学习挑战及前人的对策**
#### **1.lack of data labels with poor data quality**
>  Thus, the main problem in this setting is lack of data labels with poor data quality.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=13&selection=2,63,3,70|CAIE_A review of applications in federated learning_deposit, page 13]]

#### **2.communication efficiency**
> Sharma et al., (2019) work hard on improvement for FTL. They made use of secret sharing technology instead of HE to further reduce overhead without decreasing the accurate rate.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=13&selection=13,42,15,61|CAIE_A review of applications in federated learning_deposit, page 13]]



## **4.Federated Averaging(FedAvg)**
[What is federated averaging (FedAvg)?](https://www.educative.io/answers/what-is-federated-averaging-fedavg)

###  **Code example**
```
import numpy as np

# Define a sample model for demonstration
class Model:
    def __init__(self):
        self.fc = np.random.randn(10, 1)
    
    def forward(self, x):
        return np.dot(x, self.fc)  

# Client-side training function
def train_local_model(client_data, model):
    num_epochs = 10
    learning_rate = 0.1

    for epoch in range(num_epochs):
        inputs, labels = client_data  
        inputs = inputs.reshape(1, -1)  
        outputs = model.forward(inputs)
        loss = np.mean((outputs - labels) ** 2)
        grad = 2 * np.dot(inputs.T, outputs - labels) / inputs.shape[1]  
        model.fc -= learning_rate * grad

    return model

# Server-side federated averaging
def federated_averaging(global_model, client_data, num_rounds):
    for round in range(num_rounds):
        # Client selection
        selected_clients = np.random.choice(range(len(client_data)), size=3, replace=False)  # Select 3 client indices
        
        # Model distribution
        client_models = [global_model] * len(selected_clients)  # Provide duplicate global model to each client
        
        # Local training
        for i, client_index in enumerate(selected_clients):
            client_model = train_local_model(client_data[client_index], client_models[i])
            client_models[i] = client_model
        
        # Model aggregation
        aggregated_model = Model()
        for client_model in client_models:
            aggregated_model.fc += client_model.fc
        
        # Model averaging
        global_model.fc = aggregated_model.fc / len(client_models)
        
        print(f"Round {round+1} - Global Model Parameters:")
        print(global_model.fc)
        print()

# Example usage
global_model = Model()
client_data = [
    (np.random.randn(10), np.random.randn(1)),
    (np.random.randn(10), np.random.randn(1)),
    (np.random.randn(10), np.random.randn(1))
]  # Dummy client data
num_rounds = 2
federated_averaging(global_model, client_data, num_rounds)
```

### **Benefits**

The federated averaging technique has the following benefits:

- Allows collaboration without providing raw data while maintaining privacy.
    
- Reduces communication costs because only model updates are sent between clients and the server.
    
- Scalable for large-scale machine learning applications.
    
- Minimizes the need for data transfer, reducing network bandwidth requirements and latency.
    
- Optimizes resource utilization, as it distributes the computational load across multiple devices or servers.
    

###  **Drawbacks**

The federated averaging has the following drawbacks:

- Data distribution heterogeneity between devices.
    
- Centralized control of the training process is lacking.
    
- Limited access to the data kept on specific servers or devices.
    
- Potential threats to privacy and security if necessary precautions are not taken.
    

### **Use cases**

The federated averaging technique is used in many different fields. It applies to:

- **Healthcare:** To train models using distributed patient data while maintaining privacy.
    
- **Finance sector:** It allows financial firms to collaborate without disclosing confidential customer data.
    
- **Edge computing relevance:** It is also helpful in edge computing applications (IoT devices, smartphones, or edge servers), where devices with a limited connection can contribute to model training.

## **5.目前的问题和解决办法**

### **5.1 Communication-efficiency**

#### **5.1.1 Local Updating**
> distributed local-updating primal-dual methods have emerged as a popular way to tackle such a problem [54 , 62 , 72 , 107 , 128 ].

[[Federated Learning Challenges, Methods, and Future Directions.pdf#page=5&selection=83,51,107,2|Federated Learning Challenges, Methods, and Future Directions, page 5]]

#### **5.1.2 Compression Schemes**
> forcing the updating models to be sparse and low-rank; performing quantization with structured random rotations [ 59 ]; using lossy compression and dropout to reduce server-to-device communication [ 15 ]; and applying Golomb lossless encoding [ 99 ].

[[Federated Learning Challenges, Methods, and Future Directions.pdf#page=6&selection=37,66,51,2|Federated Learning Challenges, Methods, and Future Directions, page 6]]

#### **5.2.3 Decentralized Training**
> Finally, hierarchical communication patterns have also been proposed [ 68 , 70 ] to further ease the burden on the central server, by first leveraging edge servers to aggregate the updates from edge devices and then relying on a cloud server to aggregate updates from edge servers. 

[[Federated Learning Challenges, Methods, and Future Directions.pdf#page=6&selection=96,3,115,9|Federated Learning Challenges, Methods, and Future Directions, page 6]]

#### **5.2.4 Reducing communication rounds**
> The research of McMahan et al., (2017) is considered as the pioneering work on FL to make communication more efficient by increasing calculated quantity on each client between each communication round. They also pointed out that increase parallelism which means motivate more clients to join training on each round is an effective way

[[CAIE_A review of applications in federated learning_deposit.pdf#page=15&selection=24,24,28,42|CAIE_A review of applications in federated learning_deposit, page 15]]

>  Inspired by Google, Nishio and Yonetani (2019) built FedCs framework to integrate the available clients to the utmost extent in each training round to make it efficiently in practice

[[CAIE_A review of applications in federated learning_deposit.pdf#page=15&selection=28,42,30,52|CAIE_A review of applications in federated learning_deposit, page 15]]

> Maximum mean discrepancy was inserted to FL algorithm to enforce local model to acquire more knowledge from other in training devices thus speed up convergence (Yao et al, 2018).

[[CAIE_A review of applications in federated learning_deposit.pdf#page=15&selection=30,52,30,80|CAIE_A review of applications in federated learning_deposit, page 15]]

> Yurochkin et al., (2019) designed Bayesian Nonparametric FL framework, which is state of the art since it can aggregate local models into a federated model without extra parameters thus avoid unwanted communication rounds. 

[[CAIE_A review of applications in federated learning_deposit.pdf#page=16&selection=1,69,4,42|CAIE_A review of applications in federated learning_deposit, page 16]]

#### **5.2.5 Decrease model update time**
##### **1.structured update**
> transmit only part of the update model by means of low-rank model or in a random mask way. 

[[CAIE_A review of applications in federated learning_deposit.pdf#page=16&selection=14,32,19,40|CAIE_A review of applications in federated learning_deposit, page 16]]

> an end-to-end neural network is a kind of structured update mode which maps update information into a lower- dimension space thus relieve pressure of communication (Li and Han, 2019).

[[CAIE_A review of applications in federated learning_deposit.pdf#page=16&selection=19,50,21,74|CAIE_A review of applications in federated learning_deposit, page 16]]

#### **2.sketched update**
>  refer to make use of compressed update model.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=16&selection=22,31,22,77|CAIE_A review of applications in federated learning_deposit, page 16]]

>  Zhu and Jin (2019) optimized sparse evolutionary training (SET) thus convey only piece of parameters to server, which resemble the sketched update. 

[[CAIE_A review of applications in federated learning_deposit.pdf#page=16&selection=22,77,24,61|CAIE_A review of applications in federated learning_deposit, page 16]]

>  Jiang and Ying (2020) designed an adaptive method for local training. The local training epochs is decided by server according to training time and training loss, thus it will reduce local training time when loss is getting small.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=16&selection=25,37,28,14|CAIE_A review of applications in federated learning_deposit, page 16]]

> Liu et al., (2020) utilized momentum gradient descent to consider previous gradient information in each local training epoch to accelerate convergence speed.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=16&selection=30,11,31,82|CAIE_A review of applications in federated learning_deposit, page 16]]


### **5.2 Systems Heterogeneity**

#### **5.2.1 Asynchronous Communication**
> bounded-delay assumptions 

[[Federated Learning Challenges, Methods, and Future Directions.pdf#page=7&selection=81,36,81,61|Federated Learning Challenges, Methods, and Future Directions, page 7]]

#### **5.2.2 Active Sampling **
>  actively selecting participating devices

[[Federated Learning Challenges, Methods, and Future Directions.pdf#page=7&selection=132,95,136,31|Federated Learning Challenges, Methods, and Future Directions, page 7]]

#### **5.2.3 Fault Tolerance**
> One practical strategy is to simply ignore such device failure [ 11], which may introduce bias into the device sampling scheme if the failed devices have specific data characteristics.

[[Federated Learning Challenges, Methods, and Future Directions.pdf#page=8&selection=44,3,49,49|Federated Learning Challenges, Methods, and Future Directions, page 8]]

> Coded computation is another option to tolerate device failures by introducing algorithmic redundancy.

[[Federated Learning Challenges, Methods, and Future Directions.pdf#page=8&selection=71,1,73,84|Federated Learning Challenges, Methods, and Future Directions, page 8]]

>  Enable FL system to be robust to dropped participants, scholars also designed secure aggregation protocol (Haoa et al., 2019) which is tolerant with arbitrary dropouts as long as surviving users are enough to join federate update.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=19&selection=16,23,21,81|CAIE_A review of applications in federated learning_deposit, page 19]]

> Lib et al., (2019) take stragglers into account and allow these devices to implement different locally update computation times. 

[[CAIE_A review of applications in federated learning_deposit.pdf#page=19&selection=21,82,26,34|CAIE_A review of applications in federated learning_deposit, page 19]]

> Wu et al., (2019) also fully considered device straggling phenomenon in heterogeneous network. They made use of a cache structure to store those unreliable user update thus alleviates their trustless impact on global model.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=19&selection=26,35,27,48|CAIE_A review of applications in federated learning_deposit, page 19]]

#### **5.2.4 Resource allocation**
>  Kang et al., (2019) took overhead in heterogeneous clients into consideration to motivate more high-quality devices to participate training process.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=19&selection=34,77,36,53|CAIE_A review of applications in federated learning_deposit, page 19]]

> Tran et al., (2019) studied training accuracy and convergence time with influence of heterogeneous power constraints.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=19&selection=36,58,38,12|CAIE_A review of applications in federated learning_deposit, page 19]]

> Chai et al., (2019) considered the impact of resource (e.g. CPU, memory, and network resources) heterogeneity on training time of FL.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=19&selection=38,24,39,73|CAIE_A review of applications in federated learning_deposit, page 19]]

>  Li, T. et al., (2020) designed a fairness metrics to measure loss in devices and a q-Fair optimization goal to impel fair resource allocation in FL. 

[[CAIE_A review of applications in federated learning_deposit.pdf#page=19&selection=40,19,40,88|CAIE_A review of applications in federated learning_deposit, page 19]]

### **5.3 Statistical Heterogeneity**

#### **5.3.1 Modeling Heterogeneous Data **
> MOCHA [ 106 ], an optimization framework designed for the federated setting, can allow for personalization by learning separate but related models for each device while leveraging a shared representation via multi-task learning.

[[Federated Learning Challenges, Methods, and Future Directions.pdf#page=8&selection=160,1,173,88|Federated Learning Challenges, Methods, and Future Directions, page 8]]

>  Another approach [ 26] models the star topology as a Bayesian network and performs variational inference during learning.

[[Federated Learning Challenges, Methods, and Future Directions.pdf#page=9&selection=22,64,26,82|Federated Learning Challenges, Methods, and Future Directions, page 9]]

>  q-FFL in which devices with higher loss are given higher relative weight to encourage less variance in the final accuracy distribution. 

[[Federated Learning Challenges, Methods, and Future Directions.pdf#page=9&selection=77,0,80,59|Federated Learning Challenges, Methods, and Future Directions, page 9]]

#### **5.3.2 Convergence Guarantees for Non-IID Data **
> FedProx [ 65 ] has recently been proposed

[[Federated Learning Challenges, Methods, and Future Directions.pdf#page=10&selection=47,1,54,28|Federated Learning Challenges, Methods, and Future Directions, page 10]]

#### **5.3.3 Convergence speed of a FL algorithm**
> urther Wang et al., (2019) discussed convergence bound of FL based on gradient-descent in Non-IID data background, and further bring forward an improved adaptive method to reduce loss function within constraints of resource budget.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=17&selection=26,33,29,16|CAIE_A review of applications in federated learning_deposit, page 17]]

> Li, X. et al (2019). gave four kinds of convergence theorems with different parameters setting or premises for FedAvg in Non-IID situations

[[CAIE_A review of applications in federated learning_deposit.pdf#page=17&selection=29,28,30,76|CAIE_A review of applications in federated learning_deposit, page 17]]

#### **5.3.4 Add extra data preprocessing procedure**
>  Huanga et al., (2019) introduced clustering thought with FL and constructed a community-based FL method. 

[[CAIE_A review of applications in federated learning_deposit.pdf#page=18&selection=5,26,10,9|CAIE_A review of applications in federated learning_deposit, page 18]]

> In hierarchical heterogeneous horizontal framework, it projects each embedding submanifold into a common embedding space to overcome data heterogeneity (Gao et al., 2019).

[[CAIE_A review of applications in federated learning_deposit.pdf#page=18&selection=15,31,17,47|CAIE_A review of applications in federated learning_deposit, page 18]]

#### **5.3.5 Modify local training mode**
>  Another idea is to optimize modeling way to achieve personalization for individual devices such as MOCHA, which introduced multi-task learning to make utilization of shared representation (Smith et al., 2017). 

[[CAIE_A review of applications in federated learning_deposit.pdf#page=18&selection=20,1,22,76|CAIE_A review of applications in federated learning_deposit, page 18]]

> Zhao et al., (2018) did the similar work, they considered a solution to deal with non-iid data by sharing a small set of data among each local model.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=18&selection=22,76,24,51|CAIE_A review of applications in federated learning_deposit, page 18]]

>  Huangb et al., (2019) also gained a good deal of enlightenment from the previous data sharing ideology to overcome Non-IID problem. 

[[CAIE_A review of applications in federated learning_deposit.pdf#page=18&selection=24,51,29,18|CAIE_A review of applications in federated learning_deposit, page 18]]


### **5.4 Privacy**

#### **5.4.1 Privacy in Machine Learning**
> differential privacy to communicate noisy data sketches, homomorphic encryption to operate on encrypted data

[[Federated Learning Challenges, Methods, and Future Directions.pdf#page=10&selection=175,8,176,14|Federated Learning Challenges, Methods, and Future Directions, page 10]]

> or gradient-based learning methods, a popular approach is to apply differential privacy by randomly perturbing the intermediate output at each iteration

[[Federated Learning Challenges, Methods, and Future Directions.pdf#page=10&selection=219,4,220,91|Federated Learning Challenges, Methods, and Future Directions, page 10]]

#### **5.4.2 Privacy in Federated Learning**
> a relaxed version of local privacy by limiting the power of potential adversaries. It affords stronger privacy guarantees than global privacy, and has better model performance than strict local privac

[[Federated Learning Challenges, Methods, and Future Directions.pdf#page=12&selection=10,10,12,24|Federated Learning Challenges, Methods, and Future Directions, page 12]]

>  locally differentially-private algorithms in the context of meta-learning, which can be applied to federated learning with personalization, while also providing provable learning guarantees in convex setting

[[Federated Learning Challenges, Methods, and Future Directions.pdf#page=12&selection=16,7,18,46|Federated Learning Challenges, Methods, and Future Directions, page 12]]

#### **5.4.3  Privacy Risk**

##### **5.4.3.1 Data poisoning attack**
> On the basis research of Bagdasaryan et al., (2018), Yang, Qb. et al., (2019) studied a novel and effective distributed backdoor attack. They divided an attack trigger into many slices and embedded each slice into different attackers instead of embedding a complete trigger into only one attacker

[[CAIE_A review of applications in federated learning_deposit.pdf#page=21&selection=12,7,17,51|CAIE_A review of applications in federated learning_deposit, page 21]]

##### **5.4.3.2 Model poisoning（Also known as Adversarial attack)**
> it can be subdivided into Non-targeted adversarial attack and Targeted adversarial attack

[[CAIE_A review of applications in federated learning_deposit.pdf#page=21&selection=26,20,27,27|CAIE_A review of applications in federated learning_deposit, page 21]]

> In FL, secure aggregation is implemented, and aggregator is not familiar with the local update modes thus are not able to detect anomalies or verify correctness of local updates

[[CAIE_A review of applications in federated learning_deposit.pdf#page=21&selection=29,25,31,35|CAIE_A review of applications in federated learning_deposit, page 21]]

> This novel attack method can be successfully employed in federated training tasks including image classification and word prediction (Bagdasaryan et al., 2018).

[[CAIE_A review of applications in federated learning_deposit.pdf#page=21&selection=33,60,34,79|CAIE_A review of applications in federated learning_deposit, page 21]]

> Zhang et al., (2019) give first attempt to generate model poisoning attack based on Generative Adversarial Nets (GAN). 

[[CAIE_A review of applications in federated learning_deposit.pdf#page=22&selection=5,67,7,13|CAIE_A review of applications in federated learning_deposit, page 22]]

##### **5.4.3.3 Inferring attack**
>  Wang, Z. et al., (2019) built a general attack frame called mGAN-AI which could reconstruct private information for target client.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=22&selection=26,40,27,83|CAIE_A review of applications in federated learning_deposit, page 22]]

##### **5.4.3.4  privacy-preserving technology in FL**
>  Insider adversaries including honest-but-curious aggregator, colluding parties and malicious participants steal privacy during training process

[[CAIE_A review of applications in federated learning_deposit.pdf#page=23&selection=6,80,8,50|CAIE_A review of applications in federated learning_deposit, page 23]]

##### **5.4.3.5 Privacy-preserving at client side**
> since FedAvg is prone to be violated by differential attack, Geyer et al., (2019) leveraged differential privacy on FL to conceal whether a client participant in the training process.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=23&selection=24,63,26,70|CAIE_A review of applications in federated learning_deposit, page 23]]

> Faced with these vulnerabilities, the existing privacy-preserving methods to enhance privacy guarantees mainly focus on information encryption for client or secure aggregation at server side as well as security protection for FL framework (Ma et al., 2019).

差分攻击(DP)是一种通过分析模型输出的微小差异来推断个别客户的数据的攻击方式。

[[CAIE_A review of applications in federated learning_deposit.pdf#page=23&selection=13,50,16,55|CAIE_A review of applications in federated learning_deposit, page 23]]

>  to improve FedAvg, McMahan et al., (2018) also applied DP to this process by adding Gaussian noise to the global model. 

[[CAIE_A review of applications in federated learning_deposit.pdf#page=23&selection=26,80,28,36|CAIE_A review of applications in federated learning_deposit, page 23]]

> In federated online training for ranker using feedback from users, Kharitonov (2019) introduced ε-local differential privacy. Opposite to common algorithms

ε-局部差分隐私通常涉及到在本地计算中引入噪声或随机性，以增强个体隐私的保护。该方法更为严格，因为它在用户级别（个体级别）保护隐私，而不是在数据聚合后才应用隐私保护技术。通常，常见的隐私保护方法可能在数据聚合或处理后应用某种隐私保护技术，而"ε-局部差分隐私"更早地在个体级别引入随机性或噪声，以确保用户的隐私得到更加严格的保护。这种方法的目标是在数据处理的早期阶段就保护隐私，而不是依赖于后期的隐私保护措施。

[[CAIE_A review of applications in federated learning_deposit.pdf#page=23&selection=28,37,28,81|CAIE_A review of applications in federated learning_deposit, page 23]]

> Ilias and Georgios (2019) also added homomorphic encryption to a more robust FL framework, which make it possible to compute aggregation on encrypted client

[[CAIE_A review of applications in federated learning_deposit.pdf#page=24&selection=12,10,14,6|CAIE_A review of applications in federated learning_deposit, page 24]]

>  Lee et al., (2018) make use of LSH to detect similar patients in federated settings.

局部敏感哈希（LSH）也是保持机密性的一种普遍方法（Gionis等人，1999年）。所有特征将通过p-稳定哈希函数映射为加密形式。这种加密模式的主要优势是，哈希表示后两个样本之间的相似性将被保留。这对于数据分析和隐私保护非常重要。这使得可以在保持隐私的同时进行一些数据关联或相似性比较的操作。

[[CAIE_A review of applications in federated learning_deposit.pdf#page=24&selection=26,35,28,31|CAIE_A review of applications in federated learning_deposit, page 24]]

##### **5.4.3.6 Secure aggregation**
secure multiparty computation

这种协议的目标是允许多个参与方在不泄露各自私密数据的情况下对数据进行聚合处理，从而保护数据隐私和安全。

>  Haoa et al., (2019) envisioned a more efficient privacy-preserving scheme for FL, which integrate differential privacy and lightweight homomorphic encryption technology. This protocol, mainly for stochastic gradient descent approach, is robust to curious-but-honest server and collusion between the cloud and server.

差分隐私用于随机化数据以保护隐私，而轻量级同态加密技术允许在加密状态下执行一些计算。

[[CAIE_A review of applications in federated learning_deposit.pdf#page=25&selection=13,37,18,48|CAIE_A review of applications in federated learning_deposit, page 25]]

VerifyNet协议具有独特的能力，可以验证从云端返回的机器学习模型的正确性。这对于确保云端计算结果的准确性和可信度非常重要，尤其是在联邦学习等分布式计算环境中。通过VerifyNet协议，可以增强对模型返回结果的信任，有助于提高云端计算的安全性和可靠性。

> The up-to-date approach proposed by Chen et al., (2020) also concentrated on secure aggregation scheme. They add an extra public parameter dispatch to each client to force them training in a same way, thus detect malicious client easily when making an aggregation stage.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=25&selection=26,48,29,69|CAIE_A review of applications in federated learning_deposit, page 25]]

##### **5.4.3.7 Protection method for FL framework**
MPC（多方计算，Secure Multi-Party Computation）是一种密码学协议和隐私保护技术，用于多个参与方之间在不暴露各自私密输入的情况下进行计算。MPC的主要目标是允许多个参与方共同执行计算任务，而不需要他们透露他们的输入数据给其他参与方或第三方。

MPC的关键特点和目标包括：

1. **隐私保护：** MPC旨在确保每个参与方的输入数据保持隐私，其他参与方无法获得对输入数据的详细了解。
    
2. **计算结果：** 尽管各方的输入数据是私密的，但MPC允许他们共同计算出某个函数的结果，而不泄露输入数据。
    
3. **安全性：** MPC协议提供了数学和密码学保障，确保即使一部分参与方是恶意的，也不会泄漏私密信息或破坏计算结果的正确性。

> To reduce noise, the Hybrid-One scheme combine the use of DP with MPC without compromising accuracy rate, which protect communication messages rely on MPC thus introduce less noise than traditional local DP (Truex et al.,2019)

混合型一方案（Hybrid-One scheme）通过将差分隐私（DP）与多方计算（MPC）相结合，来降低在隐私保护过程中引入的噪声。与传统的本地差分隐私相比，这种组合方式有助于保护通信消息的隐私，同时不会牺牲准确性。这是一种旨在在隐私和数据准确性之间找到平衡的方法，特别适用于需要同时保护数据隐私和维持高准确性的场景。

[[CAIE_A review of applications in federated learning_deposit.pdf#page=26&selection=7,0,9,71|CAIE_A review of applications in federated learning_deposit, page 26]]

> Then the efficient HybridAlpha emerged at the right moment, which combined functional encryption with SMC protocol to achieve the highly-performance model without privacy sacrifice (Xu et al.,2019).

HybridAlpha方案，它是一种高效的隐私保护方法。该方案结合了功能性加密（functional encryption）和SMC（多方计算）协议，以实现在保护隐私的同时获得高性能的机器学习模型。这种方法的出现意味着可以在不牺牲数据隐私的情况下实现高效的计算和模型训练，这对于需要在隐私敏感环境中进行数据分析和机器学习任务的应用非常有价值。

[[CAIE_A review of applications in federated learning_deposit.pdf#page=26&selection=11,32,13,68|CAIE_A review of applications in federated learning_deposit, page 26]]

> Liu, Li, Smith and Sekar (2019) established relationship between FL and sketching algorithm to strength confidentiality.

草图算法是一种不需要存储原始数据身份信息的数据处理技术，这与联邦学习的隐私保护目标相吻合。由于草图算法的特性，它可以帮助提高联邦学习中的数据保密性，同时减少了对原始数据身份的追踪需求。这种关联为在联邦学习中使用草图算法提供了新的可能性，有助于更好地平衡隐私和数据分析的需求。

[[CAIE_A review of applications in federated learning_deposit.pdf#page=26&selection=15,79,17,28|CAIE_A review of applications in federated learning_deposit, page 26]]

## **6.应用**

### **6.1 Application for mobile devices**
predict users’ input

Further improvement for prediction on keyboard

emoji prediction

predict human trajectory (Feng et al., 2020) or human behavior (Sozinov et al., 2018)

> combination of FL and MEC, Wang, X. et al., (2019) investigate an ‘In-Edge AI’ framework which combine FL based on deep reinforcement learning with MEC system and further optimize resource allocation problem. 
> 深度强化学习的联邦学习用于在分布式边缘计算环境中进行模型训练和优化，以解决资源分配问题。这种结合可以提高计算效率，减少数据传输延迟，并更好地利用边缘计算资源，有助于实现更智能的边缘计算应用。

MEC（移动边缘计算）允许在边缘设备上进行计算和数据处理，以减少延迟和提高性能，但与之相关的数据处理也增加了信息泄漏的风险。

[[CAIE_A review of applications in federated learning_deposit.pdf#page=27&selection=24,36,31,9|CAIE_A review of applications in federated learning_deposit, page 27]]

> Aïvodji et al., (2019) present a sufficient secure federated architecture to build joint models. 

[[CAIE_A review of applications in federated learning_deposit.pdf#page=28&selection=6,0,7,8|CAIE_A review of applications in federated learning_deposit, page 28]]

> Yu et al., (2020) build a federated multi-task learning framework for smart home IOT to automatically learn users’ behavior patterns, which could effectively detect physical hazards.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=28&selection=7,20,11,36|CAIE_A review of applications in federated learning_deposit, page 28]]

> Liu, B. et al., (2020) proposed a data fusion approach based on FL for robots imitation learning in robot networking.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=28&selection=11,50,12,78|CAIE_A review of applications in federated learning_deposit, page 28]]

### **6.2 Application in Industrial Engineering**
> Hu et al., (2018) designed a novel environmental monitoring frame based on federated region learning FRL) for the sake of inconvenient interchangeable monitor data.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=28&selection=25,72,27,72|CAIE_A review of applications in federated learning_deposit, page 28]]

> L is also applied to visual inspection task (Han et al.,2019).It could not only help us solve the problem of lacking defective samples to detect defects in production tasks but also offered privacy guarantees for manufacturers.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=28&selection=29,37,29,85|CAIE_A review of applications in federated learning_deposit, page 28]]

visual inspection task

malicious attacks detection in communication system composed by Unmanned Aerial Vehicles (UAVs)

> With the popularization of electric vehicles, Saputra et al., (2019) designed a federated energy demand prediction method for various charging stations to prevent energy congestion in transmission process

[[CAIE_A review of applications in federated learning_deposit.pdf#page=29&selection=8,67,11,23|CAIE_A review of applications in federated learning_deposit, page 29]]

>  Yang, Zhang, Ye, Li and C.-Z. Xu (2019) leveraged FL to transactions owned by different banks in order to detect credit card fraud efficiently, which is also a significant contribution to financial field.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=29&selection=11,34,13,79|CAIE_A review of applications in federated learning_deposit, page 29]]

>  For text mining, Wang, Y. et al., (2020) exploit an industrial grade federated framework based on Latent Dirichlet Allocation. It has passed the assessment on real data for spam filtering and sentiment analysis.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=29&selection=13,79,16,33|CAIE_A review of applications in federated learning_deposit, page 29]]

### **6.3 Application in HealthCare**
> Kim, et al., (2017) gave an attempt to use tensor factorization models for phenotyping analysis to obtain information concealed in health records without sharing patient- level data.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=30&selection=6,0,8,11|CAIE_A review of applications in federated learning_deposit, page 30]]

>  Huanga et al., (2019) make use of EMRs scattered across hospitals to predict mortality rate for heart disease patients. During training process, there is not any form of data or parameters transmission among hospitals’ databases. 

[[CAIE_A review of applications in federated learning_deposit.pdf#page=30&selection=11,33,19,11|CAIE_A review of applications in federated learning_deposit, page 30]]

NLP based on FL

> Liu, Dligach and Miller (2019) focus on need for unstructured data processing of clinical notes.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=31&selection=5,76,9,6|CAIE_A review of applications in federated learning_deposit, page 31]]

> Federated principal components analysis (fPCA) has been put forward by Silva et al., (2019) to extract features from magnetic resonance images (MRI) come from different medical centers.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=31&selection=15,0,17,26|CAIE_A review of applications in federated learning_deposit, page 31]]

> Gao et al., (2019) proposed a hierarchical heterogeneous horizontal FL (HHHFL) framework for Electroencephalography (EEG) classification to overcome the challenge of limited labeled instances as well as the privacy constraint.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=31&selection=17,40,20,23|CAIE_A review of applications in federated learning_deposit, page 31]]

## **7.未来方向**

### **7.1 Extreme communication schemes**

### **7.2 Communication reduction and the Pareto frontier**

Pareto frontier

### **7.3 Novel models of asynchrony**

device-centric communication scheme

### **7.4 Heterogeneity diagnostics**

 (i) Do simple diagnostics exist to quickly determine the level of heterogeneity in federated networks a priori?
 (ii) Can analogous diagnostics be developed to quantify the amount of systems-related heterogeneity? 
 (iii) Can current or new definitions of heterogeneity be exploited to further improve the convergence of federated optimization methods?

### **7.5 Granular privacy constraints**

Developing methods to handle mixed (device-specific or sample-specific) privacy restrictions

### **7.5 Beyond supervised learning**

scalability, heterogeneity, and privacy

### **7.6 Productionizing federated learning**

concept drift(when the underlying data-generation model changes over time)
diurnal variations(when the devices exhibit different behavior at different times of the day or week)
cold start problems(when new devices enter the network)

### **7.7 Benchmarks**

### **7.8 Security compliance establishment**

### **7.9 Attack defense and efficiency promotion**

### **7.10 Asynchronous training mode**
> the synchronous training has already become the major form for FL due to superior performance of SGD in the central server settings compared to asynchronous way (Chen, Ning and Rangwala, 2019; Mohammad and Sorour, 2019) Prior optimization of FL mainly

[[CAIE_A review of applications in federated learning_deposit.pdf#page=32&selection=43,62,49,74|CAIE_A review of applications in federated learning_deposit, page 32]]

>  asynchronous online FL framework presented by Chen et al., (2019) updates central model in an asynchronous way by introducing feature learning and dynamic learning step size.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=33&selection=22,29,26,40|CAIE_A review of applications in federated learning_deposit, page 33]]

> Wu et al., (2019) proposed a semi- asynchronous protocol which allow straggling clients don’t always go together with central server.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=33&selection=28,46,32,15|CAIE_A review of applications in federated learning_deposit, page 33]]

### **7.11 Gradient aggregation**
> Yao et al., (2019) keep trace of dispatched global parameters in each local training epoch. Since local gradient update is a function of global parameters, then gradients can be aggregated in an unbiased way. 

[[CAIE_A review of applications in federated learning_deposit.pdf#page=34&selection=8,75,14,32|CAIE_A review of applications in federated learning_deposit, page 34]]

>   Ji et al., (2019) introduce a recurrent neural network aggregator to automatically get an optimized way for gradient aggregation. 

[[CAIE_A review of applications in federated learning_deposit.pdf#page=34&selection=14,73,18,40|CAIE_A review of applications in federated learning_deposit, page 34]]

> Wang et al., (2019) designed a layer-wise aggregation mode to serially generate layer parameters in neural network for global model. 

[[CAIE_A review of applications in federated learning_deposit.pdf#page=34&selection=18,53,25,18|CAIE_A review of applications in federated learning_deposit, page 34]]

### **7.11 Incentive mechanism**
> Sarikaya and Ercetin (2019) explore inventive mechanism in Stackelberg perspective to inspire workers to allocate more CPU for local training

[[CAIE_A review of applications in federated learning_deposit.pdf#page=35&selection=2,30,6,14|CAIE_A review of applications in federated learning_deposit, page 35]]

>  Khan et al., (2019) discussed Stackelberg-based incentive mechanism to set local iteration times adaptively to be effective as much as possible.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=35&selection=6,14,8,76|CAIE_A review of applications in federated learning_deposit, page 35]]

### **7.12 Verification for returned model**
>   Li  et al., (2019) considered an autoencoder enable model parameters to be replaced by low-dimension vector as well as discover irregular weights update

[[CAIE_A review of applications in federated learning_deposit.pdf#page=35&selection=39,41,43,35|CAIE_A review of applications in federated learning_deposit, page 35]]

>   Muñoz-González and Lupu (2019) discussed adaptive FL to grub abnormal updates via a Hidden Markov Model to evaluate model quality.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=35&selection=43,35,47,8|CAIE_A review of applications in federated learning_deposit, page 35]]

### **7.13 FL with block-chain technology**
> Hence Ilias and Georgios (2019) utilized blockchain smart convention to coordinate all clients and additionally used homomorphic encryption to provide extra privacy guarantee.

[[CAIE_A review of applications in federated learning_deposit.pdf#page=36&selection=46,66,50,76|CAIE_A review of applications in federated learning_deposit, page 36]]

> The blockchain-based privacy-preserving FL framework designed by Awan et al., (2019) also added a variation of the Paillier cryptosystem as an excess measure to forestall privacy leakage. 

[[CAIE_A review of applications in federated learning_deposit.pdf#page=37&selection=0,0,2,17|CAIE_A review of applications in federated learning_deposit, page 37]]

## 经验风险 Empirical risk
#以后可以延伸的知识/经验风险
[[Federated Learning Challenges, Methods, and Future Directions.pdf#page=3&selection=91,51,91,66|Federated Learning Challenges, Methods, and Future Directions, page 3]]
经验风险，基于训练集所有样本点损失函数的平均最小化。经验风险是**局部最优**，是**现实**的可求的。

******经验风险=经验损失=代价函数******

给定一个数据集，模型f(x)关于训练集的平均损失被称为经验风险(empirical risk)或经验损失(empirical loss)。

![](https://img2020.cnblogs.com/blog/968024/202009/968024-20200911200524332-1536630142.png)

这个公式的用意很明显，就是模型关于训练集的平均损失（每个样本的损失加起来，然后平均一下）。在实际中用的时候，我们也就很自然的这么用了。

### 经验风险最小化

**经验风险最小化（****empirical risk minimization，****ERM），**就是认为经验风险最小的模型是最优的模型，用公式表示：

![](https://img2020.cnblogs.com/blog/968024/202009/968024-20200911200938304-535333836.png)  
这个理论很符合人的直观理解。因为在训练集上面的经验风险最小，也就是平均损失越小，意味着模型得到结果和“真实值”尽可能接近，表明模型越好。

### ****经验风险最小化的例子：****极大似然估计（maximum likelihood estimation）。

模型，条件概率分布；

损失函数，对数损失函数；

经验风险最小化等价于极大似然估计。


## Model updates as part of the training process
#以后可以延伸的知识 
[[Federated Learning Challenges, Methods, and Future Directions.pdf#page=3&selection=213,1,216,31|Federated Learning Challenges, Methods, and Future Directions, page 3]]

## Stragglers
#以后可以延伸的知识/stragglers
[Straggling Workers in Distributed Computing | by Benjamin | The Startup | Medium](https://medium.com/swlh/straggling-workers-in-distributed-computing-ec60776c7a73)

### 背景知识
#### Clusters
A cluster, in this realm, is a collection of dedicated computational hardware that can communicate; usually over a dedicated private network (but keep in mind distributed systems exist over public networks).

**Single end-point nodes of this cluster** are what we call **workers **

#### stragglers的含义
In cloud computing and high performance computing, a large job is typically divided into many small tasks for parallel execution in a distributed environment. Due to different reasons, some tasks (so-called **stragglers**) are considerably slower than the others, delaying the completion of the job.

### Meta-Learning
#以后可以延伸的知识/元学习 
[一文入门元学习（Meta-Learning）（附代码） - 知乎](https://zhuanlan.zhihu.com/p/136975128)
元学习Meta Learning，含义为学会学习，即learn to learn，就是带着这种对人类这种“学习能力”的期望诞生的。Meta Learning希望使得模型获取一种“学会学习”的能力，使其可以在获取已有“知识”的基础上快速学习新的任务，如：

- 让Alphago迅速学会下象棋
- 让一个猫咪图片分类器，迅速具有分类其他物体的能力

**需要注意的是，虽然同样有“预训练”的意思在里面，但是元学习的内核区别于迁移学习（Transfer Learning）**

![[Pasted image 20230917140008.png]]

在机器学习中，**训练单位是一条数据**，通过数据来对模型进行优化；数据可以分为训练集、测试集和验证集。在元学习中，训练单位分层级了，**第一层训练单位是任务，也就是说，元学习中要准备许多任务来进行学习，第二层训练单位才是每个任务对应的数据**。

二者的目的都是找一个Function，只是两个Function的功能不同，要做的事情不一样。机器学习中的Function直接作用于特征和标签，去寻找特征与标签之间的关联；而元学习中的Function是用于寻找新的f，新的f才会应用于具体的任务。

### Multi-task learning
#以后可以延伸的知识/多任务学习 

### Data silo
#以后可以延伸的知识/数据筒仓 
[What is a Data Silo? | TIBCO Software](https://www.tibco.com/reference-center/what-is-a-data-silo)
A **data silo** is a collection of information isolated from an organization and inaccessible to all parts of a company hierarchy.

### Logistic regression
#以后可以延伸的知识/逻辑回归
[Module 4 - Logistic Regression | The Programming Foundation](https://learn.theprogrammingfoundation.org/getting_started/intro_data_science/module4/?gclid=Cj0KCQjw06-oBhC6ARIsAGuzdw2CzvjvLPVT9PNkKsYMbsat4oXbGZhbsaSNrbxh5h1SFp5HjFqc7y4aAiY0EALw_wcB)
### **What is Logistic Regression?**

The logistic regression statistic modeling technique is used when we have a binary outcome variable. For example: given the parameters, will the student pass or fail? Will it rain or not? etc.

### Homomorphic Encryption
#以后可以延伸的知识/同态加密
[What Is Homomorphic Encryption & How Is It Used | Venafi](https://venafi.com/blog/homomorphic-encryption-what-it-and-how-it-used/)
The [purpose of homomorphic encryption](https://eprint.iacr.org/2015/1192.pdf) is to allow computation on encrypted data.It apart from other forms of encryption is that it uses an algebraic system to allow a variety of computations (or operations) on the encrypted data.

In mathematics, [homomorphic](https://searchsecurity.techtarget.com/definition/homomorphic-encryption) describes the transformation of one data set into another while preserving relationships between elements in both sets.

most homomorphic encryption schemes work best with data represented as integers and while using addition and multiplication as the operational functions.

### **Why use homomorphic encryption**

Organizations can use traditional encryption methods to secure sensitive data on cloud environments. But if they need to investigate or validate encrypted data in the cloud, they would need to either decrypt the data or download it and decrypt it. The first option can lead to security problems and the second can be costly and time-consuming.

### **Types of Homomorphic Encryption**

There are three types of homomorphic encryption. The primary difference between them is related to the types and frequency of mathematical operations that can be performed on the ciphertext. The three types are:

1.Partially Homomorphic Encryption
	 Only select mathematical functions to be performed on encrypted values. This means that only one operation, either addition or multiplication, can be performed an unlimited number of times on the ciphertext.PHE with multiplicative operations is the foundation for RSA encryption, which is commonly used in establishing secure connections through SSL/TLS.
2.Somewhat Homomorphic Encryption
	A somewhat homomorphic encryption (SHE) scheme is one that supports select operation (either addition or multiplication) up to a certain complexity, but these operations can only be performed a set number of times.
3.Fully Homomorphic Encryption
	Fully homomorphic encryption (FHE) [has a lot of potential](https://www.thesslstore.com/blog/what-is-homomorphic-encryption/) for making functionality consistent with privacy by helping to keep information secure and accessible at the same time.

### Security model
#以后可以延伸的知识/安全模型
[Security model - MPC wiki](https://wiki.mpcalliance.org/security_model.html)
we should define the security goal, which is called the security model in cryptography. The semi-honest model and malicious model are widely considered to capture the capabilities of adversaries.

#### **1.Semi-Honest Adversary**

In the semi-honest setting, the players strictly follow the instructions of the protocol, and are still curious to learn information of other inputs from the transactions. This is meaningful in some enterprise-to-enterprise business scenarios, where the enterprises must strictly follow the procedure and the codes are run in some safe place that stuffs can not touch.

#### **2.Malicious Adversary**

The malicious setting (or active security) is more realistic than the semi-honest setting. In the malicious setting players can do what ever they want to obtain private information of other parties inputs. This means they can arbitrarily deviate from the protocol in any way they feel.

The malicious setting comes in two variants. In the first variant called active-security-with-abort the honest players will abort the protocol when the adversary deviates from the protocol, or more realistically the honest players abort with overwhelming probability if the adversary deviates. In the second variant, called robust security, the honest players are able to continue with the protocol even though the malicious players have deviated from the protocol.

Active-security-with-abort can be obtained no matter how many parties one has, and no matter how many adversaries. However, the second variant of robust security can only be obtained when one has an honest majority; i.e. the number of honest parties is more than the number of dishonest parties. Thus in the important case of two party multi-party computation one can only obtain active-security-with-abort.

### Stochastic gradient descent
#以后可以延伸的知识/随机梯度下降

### Cross-entropy loss
#以后可以延伸的知识/交叉熵损失
[Site Unreachable](https://machinelearningmastery.com/cross-entropy-for-machine-learning/)
Cross-entropy is a measure from the field of information theory, building upon [entropy](https://machinelearningmastery.com/what-is-information-entropy/) and generally calculating the difference between two probability distributions.It is closely related to but is different from [KL divergence](https://machinelearningmastery.com/divergence-between-probability-distributions/) that calculates the relative entropy between two probability distributions, whereas cross-entropy can be thought to calculate the total entropy between the distributions.

Cross-entropy is also related to and often confused with [logistic loss, called log loss](https://machinelearningmastery.com/logistic-regression-with-maximum-likelihood-estimation/). Although the two measures are derived from a different source, when used as loss functions for classification models, both measures calculate the same quantity and can be used interchangeably.

#### **1.What Is Cross-Entropy?**

Cross-entropy is a measure of the difference between two probability distributions for a given random variable or set of events.Lower probability events have more information, higher probability events have less information.

Information _h(x)_ can be calculated for an event _x_, given the probability of the event _P(x)_ as follows:

- h(x) = -log(P(x))

Entropy _H(x)_ can be calculated for a random variable with a set of _x_ in _X_ discrete states discrete states and their probability _P(x)_ as follows:

- H(X) = – sum x in X P(x) * log(P(x))

The intuition for this definition comes if we consider a target or underlying probability distribution P and an approximation of the target distribution Q, then the cross-entropy of Q from P is the number of additional bits to represent an event using Q instead of P.

The cross-entropy between two probability distributions, such as Q from P, can be stated formally as:

- H(P, Q)

Where H() is the cross-entropy function, P may be the target distribution and Q is the approximation of the target distribution.

Cross-entropy can be calculated using the probabilities of the events from P and Q, as follows:

- H(P, Q) = – sum x in X P(x) * log(Q(x))

Where P(x) is the probability of the event x in P, Q(x) is the probability of event x in Q and log is the base-2 logarithm, meaning that the results are in bits. If the base-e or natural logarithm is used instead, the result will have the units called nats.

This calculation is for discrete probability distributions, although a similar calculation can be used for continuous probability distributions using the integral across the events instead of the sum.

The result will be a positive number measured in bits and will be equal to the entropy of the distribution if the two probability distributions are identical.

说实话我没看懂，还是看这个吧[機器/深度學習: 基礎介紹-損失函數(loss function) | by Tommy Huang | Medium](https://chih-sheng-huang821.medium.com/%E6%A9%9F%E5%99%A8-%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-%E5%9F%BA%E7%A4%8E%E4%BB%8B%E7%B4%B9-%E6%90%8D%E5%A4%B1%E5%87%BD%E6%95%B8-loss-function-2dcac5ebb6cb)

### Data poisoning
#以后可以延伸的知识/数据污染
当涉及到数据污染攻击时，可以进一步解释这两种主要类型的攻击：

1. 模型偏差（Model Skew）攻击：
   - 模型偏差攻击旨在通过操纵训练数据来引导机器学习模型产生错误的预测或决策。
   - 攻击者可能会有意修改训练数据中的一些样本，以便模型在未来的预测中偏向攻击者所期望的结果。
   - 举例来说，如果攻击者想要迷惑一个垃圾邮件过滤器，他们可以在训练数据中添加一些正常邮件的特征，使垃圾邮件模型更容易将正常邮件误分类为垃圾邮件。

2. 反馈武器化（Feedback Weaponization）攻击：
   - 反馈武器化攻击是一种更加复杂和有针对性的攻击方式，它利用模型的输出反馈来进一步损害模型的性能或达到攻击者的目标。
   - 攻击者可能会监视模型的输出，然后有针对性地选择性地修改输入数据，以使模型的输出满足他们的意图。
   - 例如，如果攻击者针对一个电子商务推荐系统，他们可能会观察系统的建议并采取行动来引导系统向某种产品或服务提供更多的推荐，以增加其销售量或影响用户决策。

这些攻击类型都旨在破坏机器学习模型的可靠性和安全性，可能导致模型在真实世界中的应用中产生不良影响。因此，研究和开发防御数据污染攻击的方法变得至关重要，特别是在联邦学习等分布式学习环境中，模型基于多个数据源进行训练。

### Sketching algorithm
#以后可以延伸的知识/草图算法
草图算法（Sketching Algorithm）是一种用于数据降维、压缩和摘要的计算技术。它的主要目标是将大规模数据集转化为相对较小的表示形式，同时尽量保留数据的关键信息。草图算法在数据挖掘、数据压缩、近似查询和分布式计算等领域中得到广泛应用。

草图算法的特点包括：

1. **降维：** 草图算法可以将高维数据映射到低维空间，从而减小数据的维度。这有助于降低计算和存储成本。

2. **压缩：** 草图算法可以将原始数据进行压缩，减小数据的体积，同时保留数据的某些特征或结构。

3. **近似：** 草图算法提供了对原始数据的近似表示，允许在不处理完整数据集的情况下进行某些查询或分析操作。

4. **摘要：** 草图算法生成了数据的摘要，用于快速汇总数据的信息。

草图算法在大数据处理、数据挖掘、数据库查询优化和网络流量分析等方面发挥了重要作用。它们通常用于解决数据量庞大的问题，以提高计算和查询效率，同时在保留数据关键特征的基础上实现数据的压缩和摘要。

### Federated Principal Components Analysis
#以后可以延伸的知识/联邦主成分分析 
PCA（Principal Component Analysis，主成分分析）是一种常用的数据降维和特征提取技术，广泛应用于数据分析、模式识别和机器学习领域。PCA的主要目标是通过线性变换将高维数据转换为低维数据，同时保留数据中的最重要信息。这种变换通常通过找到数据的主成分来实现，这些主成分是原始数据中方差最大的线性组合。

PCA的工作原理如下：

1. **数据中心化：** 首先，对原始数据进行中心化处理，即减去数据的均值，以确保数据的均值为零。

2. **计算协方差矩阵：** 然后，计算中心化数据的协方差矩阵。协方差矩阵描述了不同特征之间的关联性和方差。

3. **特征值分解：** 对协方差矩阵进行特征值分解，得到特征值和对应的特征向量。特征向量是协方差矩阵的特征结构，表示了数据中的主要方向。

4. **选择主成分：** 根据特征值的大小，选择前k个特征向量作为主成分，其中k是降维后的维度。通常，选择的特征向量对应于最大的特征值，因为它们包含了最多的信息。

5. **投影数据：** 将原始数据投影到所选的主成分上，得到降维后的数据。

PCA的主要应用包括数据可视化、噪声去除、特征选择、数据压缩等。通过降维，PCA可以减少数据的维度，减小计算复杂度，同时保留数据中的关键信息，有助于提高模型的性能和效率。

Federated Principal Components Analysis（联邦主成分分析）是一种联邦学习（Federated Learning）的应用，用于分析和降维多个分布式数据集的方法。主成分分析（PCA）是一种常用的降维技术，用于减少数据维度并保留数据集中的关键信息。在联邦学习背景下，Federated PCA允许多个参与方协作执行PCA，而无需共享原始数据。

以下是Federated PCA的主要特点和工作原理：

1. **分布式数据：** 在联邦学习中，数据存储在多个分布式参与方的本地，不需要将数据集中到中心服务器。每个参与方都有自己的数据集。

2. **隐私保护：** Federated PCA确保了数据隐私，因为原始数据不会离开参与方的本地。模型参数和中间结果是唯一共享的信息。

3. **局部PCA：** 每个参与方执行PCA分析其本地数据，得到局部主成分。这样可以在保护隐私的同时，分析每个参与方的数据特点。

4. **聚合结果：** 参与方将局部主成分合并或聚合起来，以获得全局主成分。这些全局主成分用于描述整个数据集的结构和变化。

5. **降维和特征提取：** Federated PCA通常用于数据降维和特征提取任务。它可以帮助减少数据维度，去除冗余信息，并提取最重要的特征。

Federated PCA的应用领域包括医疗保健、金融、物联网等需要分析多个分布式数据源的领域。这种方法允许数据所有者在保护隐私的同时，合作进行数据分析和模型建设。

### Dynamic learning step size
#以后可以延伸的知识/动态学习步长
动态学习步长（Dynamic Learning Rate）是机器学习和深度学习中的一个重要概念。学习步长（Learning Rate）是一个控制模型参数更新幅度的超参数，它决定了模型在每次迭代中应该学习多少。动态学习步长是指在训练过程中根据模型的性能和训练进展动态地调整学习步长的技术。

通常情况下，学习步长是一个常数，例如0.01或0.001，它在整个训练过程中保持不变。但在某些情况下，特别是在深度神经网络的训练中，使用动态学习步长可以带来以下好处：

1. **加速收敛：** 在训练初期，模型可能需要较大的学习步长以加快收敛速度。随着训练的进行，学习步长可以逐渐减小，以更精细地调整模型参数。

2. **避免震荡：** 如果学习步长设置得太大，模型的参数可能会在最优值附近震荡，而不是稳定地收敛。动态学习步长可以减小这种震荡的可能性。

3. **自适应性：** 动态学习步长可以根据模型在每次迭代中的性能自适应地调整。如果模型的性能改善较慢，学习步长可以减小，以更小的步伐前进，反之亦然。

动态学习步长的调整可以采用各种方法，包括指数衰减、自适应方法（如Adam优化器中的自适应学习步长）和基于性能的调整。这些方法的目标是确保模型在训练过程中更有效地收敛到最佳解决方案，同时避免梯度下降中的一些问题，如梯度爆炸或梯度消失。

### Gradient Descent
#以后可以延伸的知识/梯度下降
在梯度下降（Gradient Descent）等优化算法中，可能会出现一些问题，这些问题可能会影响模型的训练效果和收敛性能。以下是一些常见的梯度下降中的问题：

1. **梯度消失（Gradient Vanishing）：** 当深度神经网络具有很多层时，梯度可能会在反向传播过程中逐渐减小到接近零，导致底层的参数几乎不会更新。这会导致底层的模型学习得非常缓慢，甚至根本无法收敛。

2. **梯度爆炸（Gradient Exploding）：** 与梯度消失相反，梯度在反向传播中可能会增长得非常快，导致参数更新的幅度过大。这会导致模型参数发散，无法获得有效的训练。

3. **局部最优解（Local Optima）：** 梯度下降可能会陷入局部最优解，而无法找到全局最优解。这在高维空间中特别常见，因为存在许多局部最优解。

4. **学习率选择（Learning Rate Selection）：** 选择不合适的学习率可能会导致训练不稳定或者收敛速度过慢。学习率太大可能导致模型不稳定，学习率太小可能导致收敛速度非常慢。

5. **过拟合（Overfitting）：** 如果梯度下降训练的轮次过多，模型可能会过度拟合训练数据，导致在新数据上的泛化性能下降。

6. **初始权重选择（Initialization）：** 初始权重的选择可能会对梯度下降的性能产生重要影响。糟糕的初始化可能导致模型陷入不稳定的状态。

为了应对这些问题，研究人员已经提出了各种改进和优化的梯度下降算法，包括随机梯度下降（SGD）、带动量的梯度下降（Momentum）、自适应学习率的方法（如Adam、RMSProp等）以及正则化技术等。这些方法旨在提高梯度下降的稳定性和效率，从而更好地训练深度学习模型。

### Stackelberg
#以后可以延伸的知识/Stackelberg
"Stackelberg" 是博弈论中的一个重要概念，用于描述一种博弈模型，其中一方的行动会影响另一方的决策，并且这两方的角色不是对等的。Stackelberg博弈是一种领导者-追随者博弈，其中有一个领导者（Leader）和一个或多个追随者（Follower）。

在Stackelberg博弈中，领导者首先做出决策，而追随者在看到领导者的决策后做出自己的决策。领导者的目标是最大化自己的利益，而追随者的目标是在领导者的决策下实现自己的最佳利益。

Stackelberg博弈的特点包括：

1. **信息不对称（Asymmetric Information）：** 领导者拥有更多的信息，他们知道追随者的反应将根据领导者的决策而变化。

2. **顺序性（Sequential）：** 领导者首先行动，然后追随者作出响应。这种顺序性是Stackelberg博弈的关键特征。

3. **领导者优势（Leader's Advantage）：** 领导者在制定决策时可以考虑追随者的反应，从而有利于他们更好地优化自己的利益。

4. **策略互动（Strategic Interaction）：** 领导者和追随者之间的策略互动是Stackelberg博弈的核心，领导者的决策会影响追随者的最佳响应。

Stackelberg博弈在经济学、管理学、工程学和其他领域中都有广泛的应用。它通常用于描述市场中的价格领导者和价格追随者之间的动态互动，也可以用于分析博弈理论中的策略性决策问题。

### Crowdsourcing
#以后可以延伸的知识/Crowdsourcing
众包（Crowdsourcing）是一种通过向广泛的人群或社区寻求外部协助、创意或劳动力来完成任务、解决问题或获取信息的方法。众包的核心思想是将一个任务或项目分发给大量在线参与者，这些参与者可以是普通公众、志愿者、独立专业人士或特定领域的专家。众包通常通过互联网平台或应用程序进行组织和管理。

众包的关键特点包括：

1. **大规模参与：** 众包利用了大规模在线社区的力量，从而可以吸引成百上千甚至成千上万的参与者，以完成各种任务。

2. **多样性：** 参与者可以具有不同的技能、背景和知识，因此众包可以用于解决各种类型的问题，从简单的数据标注到复杂的科学研究。

3. **成本效益：** 众包通常更具成本效益，因为它可以避免雇佣全职员工或专家，而是按需支付参与者。

4. **加速任务完成：** 通过将任务分发给大量参与者，众包可以加速任务的完成速度，尤其是在需要大量人力的项目中。

5. **多领域应用：** 众包可用于各种领域，包括数据标注、文本翻译、图像识别、科学研究、创意设计等。

6. **问题解决和创新：** 众包不仅用于任务执行，还可以用于创意生成和问题解决。通过众包，可以获得新的观点和创意。

众包在商业、科学研究、非营利组织和政府等各个领域都有广泛的应用。例如，众包可以用于创建地图数据、进行市场研究、改进自然语言处理算法、解决科学难题等。这种方法已经成为利用全球社区的智慧和资源来解决各种问题的有效途径。

###  Hidden Markov Mode
#以后可以延伸的知识/隐马尔可夫模型
隐马尔可夫模型（Hidden Markov Model，HMM）是一种数学统计模型，广泛用于建模具有时序性质的数据，例如语音识别、自然语言处理、生物信息学、金融分析等领域。HMM 的名称中包含了两个关键概念：

1. **马尔可夫模型（Markov Model）：** 马尔可夫模型是一种描述状态变化的数学模型，其中假设系统的未来状态只取决于当前状态，而不受过去状态的影响。这就是所谓的马尔可夫性质。马尔可夫模型通常用状态和状态转移概率来表示，可以用有向图表示状态之间的转移关系。

2. **隐（Hidden）：** 隐马尔可夫模型之所以称为“隐”，是因为它涉及到两组变量：观察变量（Observations）和隐藏状态变量（Hidden States）。观察变量是我们可以观察到或测量到的数据，而隐藏状态变量则是模型内部的状态，通常不可直接观察到，但会影响观察数据的生成过程。

HMM 通常用于以下两个主要问题：

1. **状态估计问题（State Estimation）：** 给定观察序列和模型参数，要求计算最可能的隐藏状态序列。这通常使用前向算法、后向算法或维特比算法来解决。

2. **参数学习问题（Parameter Learning）：** 给定观察序列，要求估计模型的参数，包括状态转移概率、初始状态概率和观察概率分布。这通常使用期望最大化（Expectation-Maximization，EM）算法来解决。

HMM 在自然语言处理中用于词性标注、语音识别中用于语音模型、生物信息学中用于DNA序列分析、金融领域中用于时间序列分析等多个领域都有广泛的应用。它的强大之处在于能够处理具有时序性和潜在隐藏结构的数据，并用概率的方式建模不确定性。